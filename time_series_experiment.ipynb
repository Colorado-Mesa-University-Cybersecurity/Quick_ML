{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "# import tsai\n",
    " \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from Quick.cleaning.loading import (\n",
    "    examine_dataset,\n",
    "    remove_infs_and_nans\n",
    ")\n",
    "\n",
    "from Quick.cleaning.utils import (\n",
    "    get_file_path\n",
    ")\n",
    "\n",
    "from Quick.runners.deep import (\n",
    "    run_deep_nn_experiment\n",
    ")\n",
    "\n",
    "from Quick.runners.residual import (\n",
    "    run_residual_deep_nn_experiment\n",
    ")\n",
    "\n",
    "from Quick.runners.sk import (\n",
    "    run_sk_experiment\n",
    ")\n",
    "\n",
    "from Quick.runners.torch import (\n",
    "    run_torch_nn_experiment\n",
    ")\n",
    "\n",
    "from Quick.constants import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "print(torch.backends.mps.is_available())\n",
    "print(torch.backends.mps.is_built())\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1/2: We now look at ./data/Darknet_experiments_base.csv\n",
      "\n",
      "\n",
      "Loading Dataset: ./data/Darknet_experiments_base.csv\n",
      "\tTo Dataset Cache: ./cache/Darknet_experiments_base.csv.pickle\n",
      "\n",
      "\n",
      "        File:\t\t\t\t./data/Darknet_experiments_base.csv  \n",
      "        Job Number:\t\t\t1\n",
      "        Shape:\t\t\t\t(117620, 64)\n",
      "        Samples:\t\t\t117620 \n",
      "        Features:\t\t\t64\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "data_path_1: str = './data/'\n",
    "data_path_2: str = './data/03-11/'\n",
    "data_sets_1: list = [\n",
    "    'Darknet_experiments_base.csv',\n",
    "    'airline-passengers.csv',\n",
    "]\n",
    "data_sets_2: list = [\n",
    "    'Syn.csv',\n",
    "]\n",
    "\n",
    "file_path_1: callable = get_file_path(data_path_1)\n",
    "file_path_2: callable = get_file_path(data_path_2)\n",
    "file_set_1: list = list(map(file_path_1, data_sets_1))\n",
    "file_set_2: list = list(map(file_path_2, data_sets_2))\n",
    "current_job: int = 0\n",
    "\n",
    "# dataset_1 = examine_dataset(0, file_set_1, data_sets_1)\n",
    "dataset_2 = examine_dataset(1, file_set_1, data_sets_1)\n",
    "# dataset_3 = examine_dataset(0, file_set_2, data_sets_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = time_series_1 = dataset_1['Dataset']\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_1 = dataset_1['Dataset'][['Passengers']].values.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_series_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['1949-01', 112],\n",
       "       ['1949-02', 118],\n",
       "       ['1949-03', 132],\n",
       "       ['1949-04', 129],\n",
       "       ['1949-05', 121],\n",
       "       ['1949-06', 135],\n",
       "       ['1949-07', 148],\n",
       "       ['1949-08', 148],\n",
       "       ['1949-09', 136],\n",
       "       ['1949-10', 119],\n",
       "       ['1949-11', 104],\n",
       "       ['1949-12', 118],\n",
       "       ['1950-01', 115],\n",
       "       ['1950-02', 126],\n",
       "       ['1950-03', 141],\n",
       "       ['1950-04', 135],\n",
       "       ['1950-05', 125],\n",
       "       ['1950-06', 149],\n",
       "       ['1950-07', 170],\n",
       "       ['1950-08', 170],\n",
       "       ['1950-09', 158],\n",
       "       ['1950-10', 133],\n",
       "       ['1950-11', 114],\n",
       "       ['1950-12', 140],\n",
       "       ['1951-01', 145],\n",
       "       ['1951-02', 150],\n",
       "       ['1951-03', 178],\n",
       "       ['1951-04', 163],\n",
       "       ['1951-05', 172],\n",
       "       ['1951-06', 178],\n",
       "       ['1951-07', 199],\n",
       "       ['1951-08', 199],\n",
       "       ['1951-09', 184],\n",
       "       ['1951-10', 162],\n",
       "       ['1951-11', 146],\n",
       "       ['1951-12', 166],\n",
       "       ['1952-01', 171],\n",
       "       ['1952-02', 180],\n",
       "       ['1952-03', 193],\n",
       "       ['1952-04', 181],\n",
       "       ['1952-05', 183],\n",
       "       ['1952-06', 218],\n",
       "       ['1952-07', 230],\n",
       "       ['1952-08', 242],\n",
       "       ['1952-09', 209],\n",
       "       ['1952-10', 191],\n",
       "       ['1952-11', 172],\n",
       "       ['1952-12', 194],\n",
       "       ['1953-01', 196],\n",
       "       ['1953-02', 196],\n",
       "       ['1953-03', 236],\n",
       "       ['1953-04', 235],\n",
       "       ['1953-05', 229],\n",
       "       ['1953-06', 243],\n",
       "       ['1953-07', 264],\n",
       "       ['1953-08', 272],\n",
       "       ['1953-09', 237],\n",
       "       ['1953-10', 211],\n",
       "       ['1953-11', 180],\n",
       "       ['1953-12', 201],\n",
       "       ['1954-01', 204],\n",
       "       ['1954-02', 188],\n",
       "       ['1954-03', 235],\n",
       "       ['1954-04', 227],\n",
       "       ['1954-05', 234],\n",
       "       ['1954-06', 264],\n",
       "       ['1954-07', 302],\n",
       "       ['1954-08', 293],\n",
       "       ['1954-09', 259],\n",
       "       ['1954-10', 229],\n",
       "       ['1954-11', 203],\n",
       "       ['1954-12', 229],\n",
       "       ['1955-01', 242],\n",
       "       ['1955-02', 233],\n",
       "       ['1955-03', 267],\n",
       "       ['1955-04', 269],\n",
       "       ['1955-05', 270],\n",
       "       ['1955-06', 315],\n",
       "       ['1955-07', 364],\n",
       "       ['1955-08', 347],\n",
       "       ['1955-09', 312],\n",
       "       ['1955-10', 274],\n",
       "       ['1955-11', 237],\n",
       "       ['1955-12', 278],\n",
       "       ['1956-01', 284],\n",
       "       ['1956-02', 277],\n",
       "       ['1956-03', 317],\n",
       "       ['1956-04', 313],\n",
       "       ['1956-05', 318],\n",
       "       ['1956-06', 374],\n",
       "       ['1956-07', 413],\n",
       "       ['1956-08', 405],\n",
       "       ['1956-09', 355],\n",
       "       ['1956-10', 306],\n",
       "       ['1956-11', 271],\n",
       "       ['1956-12', 306],\n",
       "       ['1957-01', 315],\n",
       "       ['1957-02', 301],\n",
       "       ['1957-03', 356],\n",
       "       ['1957-04', 348],\n",
       "       ['1957-05', 355],\n",
       "       ['1957-06', 422],\n",
       "       ['1957-07', 465],\n",
       "       ['1957-08', 467],\n",
       "       ['1957-09', 404],\n",
       "       ['1957-10', 347],\n",
       "       ['1957-11', 305],\n",
       "       ['1957-12', 336],\n",
       "       ['1958-01', 340],\n",
       "       ['1958-02', 318],\n",
       "       ['1958-03', 362],\n",
       "       ['1958-04', 348],\n",
       "       ['1958-05', 363],\n",
       "       ['1958-06', 435],\n",
       "       ['1958-07', 491],\n",
       "       ['1958-08', 505],\n",
       "       ['1958-09', 404],\n",
       "       ['1958-10', 359],\n",
       "       ['1958-11', 310],\n",
       "       ['1958-12', 337],\n",
       "       ['1959-01', 360],\n",
       "       ['1959-02', 342],\n",
       "       ['1959-03', 406],\n",
       "       ['1959-04', 396],\n",
       "       ['1959-05', 420],\n",
       "       ['1959-06', 472],\n",
       "       ['1959-07', 548],\n",
       "       ['1959-08', 559],\n",
       "       ['1959-09', 463],\n",
       "       ['1959-10', 407],\n",
       "       ['1959-11', 362],\n",
       "       ['1959-12', 405],\n",
       "       ['1960-01', 417],\n",
       "       ['1960-02', 391],\n",
       "       ['1960-03', 419],\n",
       "       ['1960-04', 461],\n",
       "       ['1960-05', 472],\n",
       "       ['1960-06', 535],\n",
       "       ['1960-07', 622],\n",
       "       ['1960-08', 606],\n",
       "       ['1960-09', 508],\n",
       "       ['1960-10', 461],\n",
       "       ['1960-11', 390],\n",
       "       ['1960-12', 432]], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(time_series_1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split for time series\n",
    "train_size = int(len(time_series_1) * 0.67)\n",
    "test_size = len(time_series_1) - train_size\n",
    "train, test = time_series_1[:train_size], time_series_1[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset, lookback, device):\n",
    "    \"\"\"Transform a time series into a prediction dataset\n",
    "    \n",
    "    Args:\n",
    "        dataset: A numpy array of time series, first dimension is the time steps\n",
    "        lookback: Size of window for prediction\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(dataset)-lookback):\n",
    "        feature = dataset[i:i+lookback]\n",
    "        target = dataset[i+1:i+lookback+1]\n",
    "        X.append(feature)\n",
    "        y.append(target)\n",
    "    return torch.tensor(X, device = device), torch.tensor(y, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([95, 1, 1]) torch.Size([95, 1, 1])\n",
      "torch.Size([47, 1, 1]) torch.Size([47, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gc/jb6dhmdd3r37x6bvx8rw9w980000gn/T/ipykernel_75785/2532977903.py:14: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1682343686209/work/torch/csrc/utils/tensor_new.cpp:248.)\n",
      "  return torch.tensor(X, device = device), torch.tensor(y, device = device)\n"
     ]
    }
   ],
   "source": [
    "lookback = 1\n",
    "X_train, y_train = create_dataset(train, lookback=lookback, device = device)\n",
    "X_test, y_test = create_dataset(test, lookback=lookback, device = device)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([86, 10, 1]) torch.Size([86, 10, 1])\n",
      "torch.Size([38, 10, 1]) torch.Size([38, 10, 1])\n",
      "tensor([[118.],\n",
      "        [132.],\n",
      "        [129.],\n",
      "        [121.],\n",
      "        [135.],\n",
      "        [148.],\n",
      "        [148.],\n",
      "        [136.],\n",
      "        [119.],\n",
      "        [104.]], device='mps:0') tensor([[132.],\n",
      "        [129.],\n",
      "        [121.],\n",
      "        [135.],\n",
      "        [148.],\n",
      "        [148.],\n",
      "        [136.],\n",
      "        [119.],\n",
      "        [104.],\n",
      "        [118.]], device='mps:0')\n",
      "tensor([[132.],\n",
      "        [129.],\n",
      "        [121.],\n",
      "        [135.],\n",
      "        [148.],\n",
      "        [148.],\n",
      "        [136.],\n",
      "        [119.],\n",
      "        [104.],\n",
      "        [118.]], device='mps:0') tensor([[129.],\n",
      "        [121.],\n",
      "        [135.],\n",
      "        [148.],\n",
      "        [148.],\n",
      "        [136.],\n",
      "        [119.],\n",
      "        [104.],\n",
      "        [118.],\n",
      "        [115.]], device='mps:0')\n",
      "tensor([[129.],\n",
      "        [121.],\n",
      "        [135.],\n",
      "        [148.],\n",
      "        [148.],\n",
      "        [136.],\n",
      "        [119.],\n",
      "        [104.],\n",
      "        [118.],\n",
      "        [115.]], device='mps:0') tensor([[121.],\n",
      "        [135.],\n",
      "        [148.],\n",
      "        [148.],\n",
      "        [136.],\n",
      "        [119.],\n",
      "        [104.],\n",
      "        [118.],\n",
      "        [115.],\n",
      "        [126.]], device='mps:0')\n",
      "tensor([[121.],\n",
      "        [135.],\n",
      "        [148.],\n",
      "        [148.],\n",
      "        [136.],\n",
      "        [119.],\n",
      "        [104.],\n",
      "        [118.],\n",
      "        [115.],\n",
      "        [126.]], device='mps:0') tensor([[135.],\n",
      "        [148.],\n",
      "        [148.],\n",
      "        [136.],\n",
      "        [119.],\n",
      "        [104.],\n",
      "        [118.],\n",
      "        [115.],\n",
      "        [126.],\n",
      "        [141.]], device='mps:0')\n",
      "tensor([[135.],\n",
      "        [148.],\n",
      "        [148.],\n",
      "        [136.],\n",
      "        [119.],\n",
      "        [104.],\n",
      "        [118.],\n",
      "        [115.],\n",
      "        [126.],\n",
      "        [141.]], device='mps:0') tensor([[148.],\n",
      "        [148.],\n",
      "        [136.],\n",
      "        [119.],\n",
      "        [104.],\n",
      "        [118.],\n",
      "        [115.],\n",
      "        [126.],\n",
      "        [141.],\n",
      "        [135.]], device='mps:0')\n",
      "tensor([[148.],\n",
      "        [148.],\n",
      "        [136.],\n",
      "        [119.],\n",
      "        [104.],\n",
      "        [118.],\n",
      "        [115.],\n",
      "        [126.],\n",
      "        [141.],\n",
      "        [135.]], device='mps:0') tensor([[148.],\n",
      "        [136.],\n",
      "        [119.],\n",
      "        [104.],\n",
      "        [118.],\n",
      "        [115.],\n",
      "        [126.],\n",
      "        [141.],\n",
      "        [135.],\n",
      "        [125.]], device='mps:0')\n",
      "tensor([[148.],\n",
      "        [136.],\n",
      "        [119.],\n",
      "        [104.],\n",
      "        [118.],\n",
      "        [115.],\n",
      "        [126.],\n",
      "        [141.],\n",
      "        [135.],\n",
      "        [125.]], device='mps:0') tensor([[136.],\n",
      "        [119.],\n",
      "        [104.],\n",
      "        [118.],\n",
      "        [115.],\n",
      "        [126.],\n",
      "        [141.],\n",
      "        [135.],\n",
      "        [125.],\n",
      "        [149.]], device='mps:0')\n",
      "tensor([[136.],\n",
      "        [119.],\n",
      "        [104.],\n",
      "        [118.],\n",
      "        [115.],\n",
      "        [126.],\n",
      "        [141.],\n",
      "        [135.],\n",
      "        [125.],\n",
      "        [149.]], device='mps:0') tensor([[119.],\n",
      "        [104.],\n",
      "        [118.],\n",
      "        [115.],\n",
      "        [126.],\n",
      "        [141.],\n",
      "        [135.],\n",
      "        [125.],\n",
      "        [149.],\n",
      "        [170.]], device='mps:0')\n",
      "tensor([[119.],\n",
      "        [104.],\n",
      "        [118.],\n",
      "        [115.],\n",
      "        [126.],\n",
      "        [141.],\n",
      "        [135.],\n",
      "        [125.],\n",
      "        [149.],\n",
      "        [170.]], device='mps:0') tensor([[104.],\n",
      "        [118.],\n",
      "        [115.],\n",
      "        [126.],\n",
      "        [141.],\n",
      "        [135.],\n",
      "        [125.],\n",
      "        [149.],\n",
      "        [170.],\n",
      "        [170.]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "lookback = 10\n",
    "X_train, y_train = create_dataset(train, lookback=lookback, device = device)\n",
    "X_test, y_test = create_dataset(test, lookback=lookback, device = device)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "for i in range(1, 10):\n",
    "    print(X_train[i], y_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AirModel(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=1, hidden_size=50, num_layers=1, batch_first=True, device = device)\n",
    "        self.linear = nn.Linear(50, 1, device = device)\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([86, 10, 1]) torch.Size([86, 10, 1]) torch.Size([38, 10, 1]) torch.Size([38, 10, 1])\n",
      "Epoch 0: train RMSE 222.7814, test RMSE 419.0745\n",
      "torch.Size([86, 10, 1]) torch.Size([86, 10, 1]) torch.Size([38, 10, 1]) torch.Size([38, 10, 1])\n",
      "Epoch 100: train RMSE 176.1330, test RMSE 370.4943\n",
      "torch.Size([86, 10, 1]) torch.Size([86, 10, 1]) torch.Size([38, 10, 1]) torch.Size([38, 10, 1])\n",
      "Epoch 200: train RMSE 139.9925, test RMSE 331.5317\n",
      "torch.Size([86, 10, 1]) torch.Size([86, 10, 1]) torch.Size([38, 10, 1]) torch.Size([38, 10, 1])\n",
      "Epoch 300: train RMSE 109.9753, test RMSE 296.9651\n",
      "torch.Size([86, 10, 1]) torch.Size([86, 10, 1]) torch.Size([38, 10, 1]) torch.Size([38, 10, 1])\n",
      "Epoch 400: train RMSE 87.2823, test RMSE 266.9706\n",
      "torch.Size([86, 10, 1]) torch.Size([86, 10, 1]) torch.Size([38, 10, 1]) torch.Size([38, 10, 1])\n",
      "Epoch 500: train RMSE 65.2312, test RMSE 235.3875\n",
      "torch.Size([86, 10, 1]) torch.Size([86, 10, 1]) torch.Size([38, 10, 1]) torch.Size([38, 10, 1])\n",
      "Epoch 600: train RMSE 49.9896, test RMSE 207.6889\n",
      "torch.Size([86, 10, 1]) torch.Size([86, 10, 1]) torch.Size([38, 10, 1]) torch.Size([38, 10, 1])\n",
      "Epoch 700: train RMSE 39.3863, test RMSE 184.2420\n",
      "torch.Size([86, 10, 1]) torch.Size([86, 10, 1]) torch.Size([38, 10, 1]) torch.Size([38, 10, 1])\n",
      "Epoch 800: train RMSE 32.1068, test RMSE 164.3498\n",
      "torch.Size([86, 10, 1]) torch.Size([86, 10, 1]) torch.Size([38, 10, 1]) torch.Size([38, 10, 1])\n",
      "Epoch 900: train RMSE 26.5490, test RMSE 146.4224\n",
      "torch.Size([86, 10, 1]) torch.Size([86, 10, 1]) torch.Size([38, 10, 1]) torch.Size([38, 10, 1])\n",
      "Epoch 1000: train RMSE 22.9798, test RMSE 131.7097\n",
      "torch.Size([86, 10, 1]) torch.Size([86, 10, 1]) torch.Size([38, 10, 1]) torch.Size([38, 10, 1])\n",
      "Epoch 1100: train RMSE 20.4849, test RMSE 119.4272\n",
      "torch.Size([86, 10, 1]) torch.Size([86, 10, 1]) torch.Size([38, 10, 1]) torch.Size([38, 10, 1])\n",
      "Epoch 1200: train RMSE 18.6871, test RMSE 108.5539\n",
      "torch.Size([86, 10, 1]) torch.Size([86, 10, 1]) torch.Size([38, 10, 1]) torch.Size([38, 10, 1])\n",
      "Epoch 1300: train RMSE 17.0868, test RMSE 100.0611\n",
      "torch.Size([86, 10, 1]) torch.Size([86, 10, 1]) torch.Size([38, 10, 1]) torch.Size([38, 10, 1])\n",
      "Epoch 1400: train RMSE 15.8620, test RMSE 93.8456\n",
      "torch.Size([86, 10, 1]) torch.Size([86, 10, 1]) torch.Size([38, 10, 1]) torch.Size([38, 10, 1])\n",
      "Epoch 1500: train RMSE 15.1640, test RMSE 89.9417\n",
      "torch.Size([86, 10, 1]) torch.Size([86, 10, 1]) torch.Size([38, 10, 1]) torch.Size([38, 10, 1])\n",
      "Epoch 1600: train RMSE 14.3270, test RMSE 86.0348\n",
      "torch.Size([86, 10, 1]) torch.Size([86, 10, 1]) torch.Size([38, 10, 1]) torch.Size([38, 10, 1])\n",
      "Epoch 1700: train RMSE 15.1441, test RMSE 89.7977\n",
      "torch.Size([86, 10, 1]) torch.Size([86, 10, 1]) torch.Size([38, 10, 1]) torch.Size([38, 10, 1])\n",
      "Epoch 1800: train RMSE 13.3301, test RMSE 86.9959\n",
      "torch.Size([86, 10, 1]) torch.Size([86, 10, 1]) torch.Size([38, 10, 1]) torch.Size([38, 10, 1])\n",
      "Epoch 1900: train RMSE 12.8715, test RMSE 82.7956\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "model = AirModel(device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = nn.MSELoss()\n",
    "loader = data.DataLoader(data.TensorDataset(X_train, y_train), shuffle=True, batch_size=8)\n",
    "\n",
    "n_epochs = 2000\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in loader:\n",
    "        y_pred = model(X_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # Validation\n",
    "    if epoch % 100 != 0:\n",
    "        continue\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_train_pred = model(X_train)\n",
    "        y_test_pred = model(X_test)\n",
    "        cpu_y_train_pred = y_train_pred.cpu()\n",
    "        cpu_y_train = y_train.cpu()\n",
    "        cpu_y_test_pred = y_test_pred.cpu()\n",
    "        cpu_y_test = y_test.cpu()\n",
    "\n",
    "        print(cpu_y_train_pred.shape, cpu_y_train.shape, cpu_y_test_pred.shape, cpu_y_test.shape)\n",
    "\n",
    "        train_rmse = np.sqrt(loss_fn(cpu_y_train_pred, cpu_y_train))\n",
    "        test_rmse = np.sqrt(loss_fn(cpu_y_test_pred, cpu_y_test))\n",
    "    print(\"Epoch %d: train RMSE %.4f, test RMSE %.4f\" % (epoch, train_rmse, test_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # shift train predictions for plotting\n",
    "    train_plot = np.ones_like(time_series_1) * np.nan\n",
    "    y_pred = model(X_train).cpu().numpy()\n",
    "    y_pred = y_pred[:, -1, :]\n",
    "    train_plot[lookback:train_size] = model(X_train)[:, -1, :].cpu().numpy()\n",
    "    # shift test predictions for plotting\n",
    "    test_plot = np.ones_like(time_series_1) * np.nan\n",
    "    test_plot[train_size+lookback:len(time_series_1)] = model(X_test)[:, -1, :].cpu().numpy()\n",
    "# plot\n",
    "plt.plot(time_series_1, c='b')\n",
    "plt.plot(train_plot, c='r')\n",
    "plt.plot(test_plot, c='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 6\n",
    "variables = 6\n",
    "test_vals = torch.cat(\n",
    "    [torch.linspace(0, 1, variables).reshape(1, -1) for _ in range(samples)],\n",
    "    dim=0\n",
    ")\n",
    "timesteps = 2\n",
    "# test_vals = torch.randn(samples, variables)\n",
    "print(test_vals)\n",
    "# we want to compile this into a time series of shape (samples/timesteps, variables, timesteps)\n",
    "test_vals_2 = torch.cat(\n",
    "    [test_vals[i:i+timesteps].permute(1,0).reshape(1, variables, timesteps) for i in range(samples - timesteps + 1)],\n",
    "    dim=0\n",
    ")\n",
    "print(test_vals_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0000, 0.2500, 0.5000, 0.7500, 1.0000],\n",
      "         [0.0000, 0.2500, 0.5000, 0.7500, 1.0000],\n",
      "         [0.0000, 0.2500, 0.5000, 0.7500, 1.0000],\n",
      "         [0.0000, 0.2500, 0.5000, 0.7500, 1.0000]]])\n",
      "tensor([[[0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2500, 0.2500, 0.2500, 0.2500],\n",
      "         [0.5000, 0.5000, 0.5000, 0.5000],\n",
      "         [0.7500, 0.7500, 0.7500, 0.7500],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000]]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "samples = 4\n",
    "variables = 5\n",
    "test_vals = torch.cat(\n",
    "    [torch.linspace(0, 1, variables).reshape(1, -1) for _ in range(samples)],\n",
    "    dim=0\n",
    ").reshape(1, samples, variables)\n",
    "timesteps = 2\n",
    "# test_vals = torch.randn(samples, variables)\n",
    "print(test_vals)\n",
    "print(test_vals.permute(0, 2,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we try to use the darknet dataset to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "darknet = dataset_2['Dataset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Protocol</th>\n",
       "      <th>Flow Duration</th>\n",
       "      <th>Total Fwd Packet</th>\n",
       "      <th>Total Bwd packets</th>\n",
       "      <th>Total Length of Fwd Packet</th>\n",
       "      <th>Total Length of Bwd Packet</th>\n",
       "      <th>Fwd Packet Length Max</th>\n",
       "      <th>Fwd Packet Length Min</th>\n",
       "      <th>Fwd Packet Length Mean</th>\n",
       "      <th>Fwd Packet Length Std</th>\n",
       "      <th>...</th>\n",
       "      <th>FWD Init Win Bytes</th>\n",
       "      <th>Bwd Init Win Bytes</th>\n",
       "      <th>Fwd Act Data Pkts</th>\n",
       "      <th>Fwd Seg Size Min</th>\n",
       "      <th>Idle Mean</th>\n",
       "      <th>Idle Std</th>\n",
       "      <th>Idle Max</th>\n",
       "      <th>Idle Min</th>\n",
       "      <th>Traffic Type</th>\n",
       "      <th>Application Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>119384497</td>\n",
       "      <td>767</td>\n",
       "      <td>2027</td>\n",
       "      <td>90681</td>\n",
       "      <td>2448831</td>\n",
       "      <td>543</td>\n",
       "      <td>0</td>\n",
       "      <td>118.228162</td>\n",
       "      <td>224.244409</td>\n",
       "      <td>...</td>\n",
       "      <td>65535</td>\n",
       "      <td>65535</td>\n",
       "      <td>166</td>\n",
       "      <td>20</td>\n",
       "      <td>1.456330e+15</td>\n",
       "      <td>3.683488e+07</td>\n",
       "      <td>1.456330e+15</td>\n",
       "      <td>1.456330e+15</td>\n",
       "      <td>Tor</td>\n",
       "      <td>audio-streaming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>119754701</td>\n",
       "      <td>1021</td>\n",
       "      <td>2537</td>\n",
       "      <td>171290</td>\n",
       "      <td>2922270</td>\n",
       "      <td>1460</td>\n",
       "      <td>0</td>\n",
       "      <td>167.766895</td>\n",
       "      <td>280.092409</td>\n",
       "      <td>...</td>\n",
       "      <td>65535</td>\n",
       "      <td>65535</td>\n",
       "      <td>292</td>\n",
       "      <td>20</td>\n",
       "      <td>1.456330e+15</td>\n",
       "      <td>3.558961e+07</td>\n",
       "      <td>1.456330e+15</td>\n",
       "      <td>1.456330e+15</td>\n",
       "      <td>Tor</td>\n",
       "      <td>audio-streaming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>118908117</td>\n",
       "      <td>683</td>\n",
       "      <td>1662</td>\n",
       "      <td>77649</td>\n",
       "      <td>2001627</td>\n",
       "      <td>543</td>\n",
       "      <td>0</td>\n",
       "      <td>113.688141</td>\n",
       "      <td>221.086482</td>\n",
       "      <td>...</td>\n",
       "      <td>65535</td>\n",
       "      <td>65535</td>\n",
       "      <td>142</td>\n",
       "      <td>20</td>\n",
       "      <td>1.456330e+15</td>\n",
       "      <td>3.301057e+07</td>\n",
       "      <td>1.456330e+15</td>\n",
       "      <td>1.456330e+15</td>\n",
       "      <td>Tor</td>\n",
       "      <td>audio-streaming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>739728</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>543</td>\n",
       "      <td>543</td>\n",
       "      <td>543</td>\n",
       "      <td>0</td>\n",
       "      <td>271.500000</td>\n",
       "      <td>383.958982</td>\n",
       "      <td>...</td>\n",
       "      <td>41180</td>\n",
       "      <td>65535</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.456330e+15</td>\n",
       "      <td>5.227810e+05</td>\n",
       "      <td>1.456330e+15</td>\n",
       "      <td>1.456330e+15</td>\n",
       "      <td>Tor</td>\n",
       "      <td>audio-streaming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>149270</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>565</td>\n",
       "      <td>565</td>\n",
       "      <td>565</td>\n",
       "      <td>0</td>\n",
       "      <td>282.500000</td>\n",
       "      <td>399.515331</td>\n",
       "      <td>...</td>\n",
       "      <td>41180</td>\n",
       "      <td>65535</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.456330e+15</td>\n",
       "      <td>1.052330e+05</td>\n",
       "      <td>1.456330e+15</td>\n",
       "      <td>1.456330e+15</td>\n",
       "      <td>Tor</td>\n",
       "      <td>audio-streaming</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Protocol  Flow Duration  Total Fwd Packet  Total Bwd packets   \n",
       "0         6      119384497               767               2027  \\\n",
       "1         6      119754701              1021               2537   \n",
       "2         6      118908117               683               1662   \n",
       "3         6         739728                 2                  2   \n",
       "4         6         149270                 2                  2   \n",
       "\n",
       "   Total Length of Fwd Packet  Total Length of Bwd Packet   \n",
       "0                       90681                     2448831  \\\n",
       "1                      171290                     2922270   \n",
       "2                       77649                     2001627   \n",
       "3                         543                         543   \n",
       "4                         565                         565   \n",
       "\n",
       "   Fwd Packet Length Max  Fwd Packet Length Min  Fwd Packet Length Mean   \n",
       "0                    543                      0              118.228162  \\\n",
       "1                   1460                      0              167.766895   \n",
       "2                    543                      0              113.688141   \n",
       "3                    543                      0              271.500000   \n",
       "4                    565                      0              282.500000   \n",
       "\n",
       "   Fwd Packet Length Std  ...  FWD Init Win Bytes  Bwd Init Win Bytes   \n",
       "0             224.244409  ...               65535               65535  \\\n",
       "1             280.092409  ...               65535               65535   \n",
       "2             221.086482  ...               65535               65535   \n",
       "3             383.958982  ...               41180               65535   \n",
       "4             399.515331  ...               41180               65535   \n",
       "\n",
       "   Fwd Act Data Pkts  Fwd Seg Size Min     Idle Mean      Idle Std   \n",
       "0                166                20  1.456330e+15  3.683488e+07  \\\n",
       "1                292                20  1.456330e+15  3.558961e+07   \n",
       "2                142                20  1.456330e+15  3.301057e+07   \n",
       "3                  0                20  1.456330e+15  5.227810e+05   \n",
       "4                  0                20  1.456330e+15  1.052330e+05   \n",
       "\n",
       "       Idle Max      Idle Min  Traffic Type  Application Type  \n",
       "0  1.456330e+15  1.456330e+15           Tor   audio-streaming  \n",
       "1  1.456330e+15  1.456330e+15           Tor   audio-streaming  \n",
       "2  1.456330e+15  1.456330e+15           Tor   audio-streaming  \n",
       "3  1.456330e+15  1.456330e+15           Tor   audio-streaming  \n",
       "4  1.456330e+15  1.456330e+15           Tor   audio-streaming  \n",
       "\n",
       "[5 rows x 64 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "darknet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "darknet_no_label = darknet.drop(columns=['Traffic Type', 'Application Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Protocol</th>\n",
       "      <th>Flow Duration</th>\n",
       "      <th>Total Fwd Packet</th>\n",
       "      <th>Total Bwd packets</th>\n",
       "      <th>Total Length of Fwd Packet</th>\n",
       "      <th>Total Length of Bwd Packet</th>\n",
       "      <th>Fwd Packet Length Max</th>\n",
       "      <th>Fwd Packet Length Min</th>\n",
       "      <th>Fwd Packet Length Mean</th>\n",
       "      <th>Fwd Packet Length Std</th>\n",
       "      <th>...</th>\n",
       "      <th>Subflow Fwd Bytes</th>\n",
       "      <th>Subflow Bwd Bytes</th>\n",
       "      <th>FWD Init Win Bytes</th>\n",
       "      <th>Bwd Init Win Bytes</th>\n",
       "      <th>Fwd Act Data Pkts</th>\n",
       "      <th>Fwd Seg Size Min</th>\n",
       "      <th>Idle Mean</th>\n",
       "      <th>Idle Std</th>\n",
       "      <th>Idle Max</th>\n",
       "      <th>Idle Min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>119384497</td>\n",
       "      <td>767</td>\n",
       "      <td>2027</td>\n",
       "      <td>90681</td>\n",
       "      <td>2448831</td>\n",
       "      <td>543</td>\n",
       "      <td>0</td>\n",
       "      <td>118.228162</td>\n",
       "      <td>224.244409</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>876</td>\n",
       "      <td>65535</td>\n",
       "      <td>65535</td>\n",
       "      <td>166</td>\n",
       "      <td>20</td>\n",
       "      <td>1.456330e+15</td>\n",
       "      <td>3.683488e+07</td>\n",
       "      <td>1.456330e+15</td>\n",
       "      <td>1.456330e+15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>119754701</td>\n",
       "      <td>1021</td>\n",
       "      <td>2537</td>\n",
       "      <td>171290</td>\n",
       "      <td>2922270</td>\n",
       "      <td>1460</td>\n",
       "      <td>0</td>\n",
       "      <td>167.766895</td>\n",
       "      <td>280.092409</td>\n",
       "      <td>...</td>\n",
       "      <td>48</td>\n",
       "      <td>821</td>\n",
       "      <td>65535</td>\n",
       "      <td>65535</td>\n",
       "      <td>292</td>\n",
       "      <td>20</td>\n",
       "      <td>1.456330e+15</td>\n",
       "      <td>3.558961e+07</td>\n",
       "      <td>1.456330e+15</td>\n",
       "      <td>1.456330e+15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>118908117</td>\n",
       "      <td>683</td>\n",
       "      <td>1662</td>\n",
       "      <td>77649</td>\n",
       "      <td>2001627</td>\n",
       "      <td>543</td>\n",
       "      <td>0</td>\n",
       "      <td>113.688141</td>\n",
       "      <td>221.086482</td>\n",
       "      <td>...</td>\n",
       "      <td>33</td>\n",
       "      <td>853</td>\n",
       "      <td>65535</td>\n",
       "      <td>65535</td>\n",
       "      <td>142</td>\n",
       "      <td>20</td>\n",
       "      <td>1.456330e+15</td>\n",
       "      <td>3.301057e+07</td>\n",
       "      <td>1.456330e+15</td>\n",
       "      <td>1.456330e+15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>739728</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>543</td>\n",
       "      <td>543</td>\n",
       "      <td>543</td>\n",
       "      <td>0</td>\n",
       "      <td>271.500000</td>\n",
       "      <td>383.958982</td>\n",
       "      <td>...</td>\n",
       "      <td>135</td>\n",
       "      <td>135</td>\n",
       "      <td>41180</td>\n",
       "      <td>65535</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.456330e+15</td>\n",
       "      <td>5.227810e+05</td>\n",
       "      <td>1.456330e+15</td>\n",
       "      <td>1.456330e+15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>149270</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>565</td>\n",
       "      <td>565</td>\n",
       "      <td>565</td>\n",
       "      <td>0</td>\n",
       "      <td>282.500000</td>\n",
       "      <td>399.515331</td>\n",
       "      <td>...</td>\n",
       "      <td>141</td>\n",
       "      <td>141</td>\n",
       "      <td>41180</td>\n",
       "      <td>65535</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.456330e+15</td>\n",
       "      <td>1.052330e+05</td>\n",
       "      <td>1.456330e+15</td>\n",
       "      <td>1.456330e+15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Protocol  Flow Duration  Total Fwd Packet  Total Bwd packets   \n",
       "0         6      119384497               767               2027  \\\n",
       "1         6      119754701              1021               2537   \n",
       "2         6      118908117               683               1662   \n",
       "3         6         739728                 2                  2   \n",
       "4         6         149270                 2                  2   \n",
       "\n",
       "   Total Length of Fwd Packet  Total Length of Bwd Packet   \n",
       "0                       90681                     2448831  \\\n",
       "1                      171290                     2922270   \n",
       "2                       77649                     2001627   \n",
       "3                         543                         543   \n",
       "4                         565                         565   \n",
       "\n",
       "   Fwd Packet Length Max  Fwd Packet Length Min  Fwd Packet Length Mean   \n",
       "0                    543                      0              118.228162  \\\n",
       "1                   1460                      0              167.766895   \n",
       "2                    543                      0              113.688141   \n",
       "3                    543                      0              271.500000   \n",
       "4                    565                      0              282.500000   \n",
       "\n",
       "   Fwd Packet Length Std  ...  Subflow Fwd Bytes  Subflow Bwd Bytes   \n",
       "0             224.244409  ...                 32                876  \\\n",
       "1             280.092409  ...                 48                821   \n",
       "2             221.086482  ...                 33                853   \n",
       "3             383.958982  ...                135                135   \n",
       "4             399.515331  ...                141                141   \n",
       "\n",
       "   FWD Init Win Bytes  Bwd Init Win Bytes  Fwd Act Data Pkts   \n",
       "0               65535               65535                166  \\\n",
       "1               65535               65535                292   \n",
       "2               65535               65535                142   \n",
       "3               41180               65535                  0   \n",
       "4               41180               65535                  0   \n",
       "\n",
       "   Fwd Seg Size Min     Idle Mean      Idle Std      Idle Max      Idle Min  \n",
       "0                20  1.456330e+15  3.683488e+07  1.456330e+15  1.456330e+15  \n",
       "1                20  1.456330e+15  3.558961e+07  1.456330e+15  1.456330e+15  \n",
       "2                20  1.456330e+15  3.301057e+07  1.456330e+15  1.456330e+15  \n",
       "3                20  1.456330e+15  5.227810e+05  1.456330e+15  1.456330e+15  \n",
       "4                20  1.456330e+15  1.052330e+05  1.456330e+15  1.456330e+15  \n",
       "\n",
       "[5 rows x 62 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "darknet_no_label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split for time series\n",
    "train_size = int(len(darknet_no_label) * 0.67)\n",
    "test_size = len(darknet_no_label) - train_size\n",
    "train, test = darknet_no_label[:train_size], darknet_no_label[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38815, 62)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78805, 62)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we standardize the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "# train-test split for time series\n",
    "# df_downsampled = darknet_no_label.sample(frac=0.1, random_state=42)\n",
    "df_downsampled = darknet_no_label.copy()\n",
    "\n",
    "scaler.fit(df_downsampled)\n",
    "df_downsampled = scaler.transform(df_downsampled)\n",
    "\n",
    "train_size = int(len(df_downsampled) * 0.67)\n",
    "test_size = len(df_downsampled) - train_size\n",
    "train, test = df_downsampled[:train_size], df_downsampled[train_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78795, 10, 62) (78795, 1, 62)\n",
      "(38805, 10, 62) (38805, 1, 62)\n",
      "torch.Size([78795, 10, 62]) torch.Size([78795, 1, 62])\n",
      "torch.Size([38805, 10, 62]) torch.Size([38805, 1, 62])\n"
     ]
    }
   ],
   "source": [
    "def create_dataset(dataset, lookback, device):\n",
    "    \"\"\"Transform a time series into a prediction dataset\n",
    "    \n",
    "    Args:\n",
    "        dataset: A numpy array of time series, first dimension is the time steps\n",
    "        lookback: Size of window for prediction\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(dataset)-lookback):\n",
    "        feature = dataset[i:i+lookback]\n",
    "        target = dataset[i+lookback:i+lookback+1]\n",
    "        X.append(feature)\n",
    "        y.append(target)\n",
    "\n",
    "\n",
    "\n",
    "    # print(len(X), len(y))\n",
    "    print((X:=np.array(X)).shape, (y := np.array(y)).shape)\n",
    "    return torch.tensor(X.astype('float32'), device=device), torch.tensor(y.astype('float32'), device=device)\n",
    "\n",
    "\n",
    "lookback = 10\n",
    "X_train, y_train = create_dataset(train, lookback=lookback, device = device)\n",
    "X_test, y_test = create_dataset(test, lookback=lookback, device = device)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "# for i in range(1, 10):\n",
    "#     print(X_train.shape)\n",
    "#     # print(X_train[i])\n",
    "#     # ts, vs = X_train[i].shape\n",
    "#     print(X_train[i])\n",
    "#     print(X_train[i].permute(1, 0))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# nn.MultiheadAttention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class AirModel(nn.Module):\n",
    "    def __init__(self, input_size, lookback_size, embedding_size=200):\n",
    "        super().__init__()\n",
    "        # self.lstm = nn.LSTM(input_size=input_size, hidden_size=embedding_size, num_layers=1, batch_first=True)\n",
    "        self.flat = nn.Flatten()\n",
    "        self.unflat = nn.Unflatten(0, (-1, 1))\n",
    "        self.cont_embed = nn.Sequential(\n",
    "            nn.Flatten(0, 1),\n",
    "            nn.Linear(input_size, embedding_size),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(embedding_size, embedding_size),\n",
    "            nn.Unflatten(0, (-1, lookback_size))\n",
    "        )\n",
    "\n",
    "        # we need a regression head to predict the next item in the sequence of shape \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(lookback*embedding_size, lookback*embedding_size),\n",
    "            # nn.BatchNorm1d(lookback_size*embedding_size),\n",
    "            # nn.LayerNorm(lookback*embedding_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(lookback*embedding_size, lookback*embedding_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(lookback*embedding_size, input_size),\n",
    "            # nn.ReLU(),\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm((lookback_size, embedding_size))\n",
    "\n",
    "        self.query = nn.Linear(embedding_size, embedding_size)\n",
    "        self.key = nn.Linear(embedding_size, embedding_size)\n",
    "        self.value = nn.Linear(embedding_size, embedding_size)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.norm2 = nn.LayerNorm(embedding_size)\n",
    "        self.norm3 = nn.LayerNorm(embedding_size)\n",
    "\n",
    "        \n",
    "\n",
    "        self.pos_embed = nn.Embedding(lookback_size, embedding_size)\n",
    "        # self.linear = nn.Linear(lookback*embedding_size, input_size)\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        # x = self.cont_embed(x)\n",
    "        x = self.cont_embed(x) + self.pos_embed(torch.arange(x.shape[1]))\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        query = self.query(x)\n",
    "        key = self.key(x)\n",
    "        value = self.value(x)\n",
    "\n",
    "        # print(query.shape, key.shape, value.shape)\n",
    "\n",
    "        pre_attention = self.softmax(torch.matmul(query, key.transpose(-2, -1)))\n",
    "        # print(pre_attention.shape)\n",
    "\n",
    "        attention = torch.matmul(pre_attention, value)\n",
    "        # print(attention.shape)\n",
    "\n",
    "        x = self.norm2(x + attention)\n",
    "\n",
    "        # print(x.shape)\n",
    "\n",
    "        # we mix the signals together\n",
    "        x = self.flat(x)\n",
    "        x = self.head(x)\n",
    "        x = self.unflat(x)\n",
    "\n",
    "        # print(x.shape)\n",
    "\n",
    "        \n",
    "        # x = self.flat(x)\n",
    "        # x, _ = self.lstm(x)\n",
    "        # # print(x.shape)\n",
    "        # x = self.flat(x)\n",
    "        # # print(x.shape)\n",
    "        # x = self.linear(x)\n",
    "        # x = self.unflat(x)\n",
    "        # # print(x.shape)\n",
    "\n",
    "        # we stop for debugging\n",
    "        # print('stopping')\n",
    "        # raise Exception('stop')\n",
    "        return x\n",
    "\n",
    "\n",
    "# model = AirModel(input_size=62, lookback_size=lookback)\n",
    "# y_pred = model(X_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class AirModel(nn.Module):\n",
    "    def __init__(self, input_size, lookback_size, device, embedding_size=200):\n",
    "        super().__init__()\n",
    "        # self.lstm = nn.LSTM(input_size=input_size, hidden_size=embedding_size, num_layers=1, batch_first=True)\n",
    "        self.embedding_size = embedding_size\n",
    "        self.flat = nn.Flatten()\n",
    "        self.unflat = nn.Unflatten(0, (-1, 1))\n",
    "        self.cont_embed = nn.Sequential(\n",
    "            nn.Flatten(0, 1),\n",
    "            nn.Linear(input_size, embedding_size, device = device),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(embedding_size, embedding_size),\n",
    "            nn.Unflatten(0, (-1, lookback_size))\n",
    "        )\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        # we need a regression head to predict the next item in the sequence of shape \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(lookback*embedding_size, lookback*embedding_size, device = device),\n",
    "            # nn.BatchNorm1d(lookback_size*embedding_size),\n",
    "            # nn.LayerNorm(lookback*embedding_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(lookback*embedding_size, lookback*embedding_size, device = device),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(lookback*embedding_size, input_size, device = device),\n",
    "            # nn.ReLU(),\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm((lookback_size, embedding_size), device = device)\n",
    "\n",
    "        self.query = nn.Linear(embedding_size, embedding_size, device = device)\n",
    "        self.key = nn.Linear(embedding_size, embedding_size, device = device)\n",
    "        self.value = nn.Linear(embedding_size, embedding_size, device = device)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.norm2 = nn.LayerNorm(embedding_size, device = device)\n",
    "        self.norm3 = nn.LayerNorm(embedding_size, device = device)\n",
    "\n",
    "        \n",
    "\n",
    "        self.pos_embed = nn.Embedding(lookback_size, embedding_size, device = device)\n",
    "        # self.linear = nn.Linear(lookback*embedding_size, input_size)\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        # x = self.cont_embed(x)\n",
    "        x = self.cont_embed(x) + self.pos_embed(torch.arange(x.shape[1], device = self.device))\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        query = self.query(x)\n",
    "        key = self.key(x)\n",
    "        value = self.value(x)\n",
    "\n",
    "        # print(query.shape, key.shape, value.shape)\n",
    "\n",
    "        pre_attention = self.softmax(torch.matmul(query, key.transpose(-2, -1))/torch.sqrt(torch.tensor(self.embedding_size, dtype=torch.float32)))\n",
    "        \n",
    "        # print(pre_attention.shape)\n",
    "\n",
    "        attention = torch.matmul(pre_attention, value)\n",
    "        # print(attention.shape)\n",
    "\n",
    "        x = self.norm2(x + attention)\n",
    "\n",
    "        # print(x.shape)\n",
    "\n",
    "        # we mix the signals together\n",
    "        x = self.flat(x)\n",
    "        x = self.head(x)\n",
    "        x = self.unflat(x)\n",
    "\n",
    "        # print(x.shape)\n",
    "\n",
    "        \n",
    "        # x = self.flat(x)\n",
    "        # x, _ = self.lstm(x)\n",
    "        # # print(x.shape)\n",
    "        # x = self.flat(x)\n",
    "        # # print(x.shape)\n",
    "        # x = self.linear(x)\n",
    "        # x = self.unflat(x)\n",
    "        # # print(x.shape)\n",
    "\n",
    "        # we stop for debugging\n",
    "        # print('stopping')\n",
    "        # raise Exception('stop')\n",
    "        return x\n",
    "\n",
    "\n",
    "# model = AirModel(input_size=62, lookback_size=lookback, device = device)\n",
    "# y_pred = model(X_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'lookback' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 82\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;66;03m# print(x.shape)\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \n\u001b[1;32m     66\u001b[0m         \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;66;03m# print('stopping')\u001b[39;00m\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;66;03m# raise Exception('stop')\u001b[39;00m\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m---> 82\u001b[0m model \u001b[38;5;241m=\u001b[39m AirModel(input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m62\u001b[39m, lookback_size\u001b[38;5;241m=\u001b[39m\u001b[43mlookback\u001b[49m)\n\u001b[1;32m     83\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model(X_batch)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lookback' is not defined"
     ]
    }
   ],
   "source": [
    "# failed model\n",
    "class AirModel(nn.Module):\n",
    "    def __init__(self, input_size, lookback_size, device, embedding_size=80, heads = 2):\n",
    "        super().__init__()\n",
    "        # self.lstm = nn.LSTM(input_size=input_size, hidden_size=embedding_size, num_layers=1, batch_first=True)\n",
    "        self.embedding_size = embedding_size\n",
    "        self.device = device\n",
    "        self.flat1 = nn.Flatten(0, 1)\n",
    "        self.flat2 = nn.Flatten()\n",
    "        self.unflat1 = nn.Unflatten(0, (-1, lookback_size, embedding_size))\n",
    "        self.unflat2 = nn.Unflatten(0, (-1, 1))\n",
    "        self.cont_embed = nn.Sequential(\n",
    "            nn.Flatten(0, 1),\n",
    "            nn.Linear(input_size, embedding_size, device = device),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(embedding_size, embedding_size),\n",
    "            nn.Unflatten(0, (-1, lookback_size))\n",
    "        )\n",
    "\n",
    "        # we need a regression head to predict the next item in the sequence of shape \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(lookback*embedding_size, lookback*embedding_size, device = device),\n",
    "            # nn.BatchNorm1d(lookback_size*embedding_size),\n",
    "            # nn.LayerNorm(lookback*embedding_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_size, embedding_size, device = device),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_size, input_size, device = device),\n",
    "            # nn.ReLU(),\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm((lookback_size, embedding_size), device = device)\n",
    "        self.norm2 = nn.LayerNorm(embedding_size, device = device)\n",
    "\n",
    "\n",
    "        self.mh_attention = nn.MultiheadAttention(embed_dim=embedding_size, num_heads=heads, dropout=0.1, device = device)\n",
    "        \n",
    "\n",
    "        self.pos_embed = nn.Embedding(lookback_size, embedding_size, device = device)\n",
    "        # self.linear = nn.Linear(lookback*embedding_size, input_size)\n",
    "    def forward(self, x):\n",
    "        batch_size, lookback_size, input_size = x.shape\n",
    "\n",
    "        x = self.cont_embed(x) + self.pos_embed(torch.arange(x.shape[1], device = self.device))\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        attention, _ = self.mh_attention(x, x, x)\n",
    "\n",
    "        x = self.norm1(x + attention)\n",
    "\n",
    "        # print(x.shape)\n",
    "\n",
    "\n",
    "        print(x.shape)\n",
    "\n",
    "        # x = x.reshape(batch_size, lookback_size, self.embedding_size)\n",
    "        # print(x.shape)\n",
    "\n",
    "\n",
    "        # we mix the signals together\n",
    "        x = self.flat2(x)\n",
    "        x = self.head(x)\n",
    "        x = self.unflat2(x)\n",
    "\n",
    "        # print(x.shape)\n",
    "\n",
    "        \n",
    "        # x = self.flat(x)\n",
    "        # x, _ = self.lstm(x)\n",
    "        # # print(x.shape)\n",
    "        # x = self.flat(x)\n",
    "        # # print(x.shape)\n",
    "        # x = self.linear(x)\n",
    "        # x = self.unflat(x)\n",
    "        # # print(x.shape)\n",
    "\n",
    "        # we stop for debugging\n",
    "        # print('stopping')\n",
    "        # raise Exception('stop')\n",
    "        return x\n",
    "\n",
    "\n",
    "model = AirModel(input_size=62, lookback_size=lookback)\n",
    "y_pred = model(X_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# failed model\n",
    "class AirModel(nn.Module):\n",
    "    def __init__(self, input_size, lookback_size, device, embedding_size=100, heads = 2):\n",
    "        super().__init__()\n",
    "        # self.lstm = nn.LSTM(input_size=input_size, hidden_size=embedding_size, num_layers=1, batch_first=True)\n",
    "        self.embedding_size = embedding_size\n",
    "        self.device = device\n",
    "        self.flat1 = nn.Flatten(0, 1)\n",
    "        self.flat2 = nn.Flatten()\n",
    "        self.unflat1 = nn.Unflatten(0, (-1, lookback_size, embedding_size))\n",
    "        self.unflat2 = nn.Unflatten(0, (-1, 1))\n",
    "        self.cont_embed = nn.Sequential(\n",
    "            nn.Flatten(0, 1),\n",
    "            nn.Linear(input_size, embedding_size, device = device),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_size, embedding_size),\n",
    "            nn.Unflatten(0, (-1, lookback_size))\n",
    "        )\n",
    "\n",
    "        # we need a regression head to predict the next item in the sequence of shape \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(lookback*embedding_size, lookback*embedding_size, device = device),\n",
    "            # nn.BatchNorm1d(lookback_size*embedding_size),\n",
    "            # nn.LayerNorm(lookback*embedding_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_size, embedding_size, device = device),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_size, input_size, device = device),\n",
    "            # nn.ReLU(),\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm((lookback_size, embedding_size), device = device)\n",
    "        self.norm2 = nn.LayerNorm(embedding_size, device = device)\n",
    "\n",
    "\n",
    "        self.mh_attention = nn.MultiheadAttention(embed_dim=embedding_size, num_heads=heads, dropout=0.1, device = device)\n",
    "        \n",
    "\n",
    "        self.pos_embed = nn.Embedding(lookback_size, embedding_size, device = device)\n",
    "        # self.linear = nn.Linear(lookback*embedding_size, input_size)\n",
    "    def forward(self, x):\n",
    "        batch_size, lookback_size, input_size = x.shape\n",
    "\n",
    "        x = self.cont_embed(x) + self.pos_embed(torch.arange(x.shape[1], device = self.device))\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        attention, _ = self.mh_attention(x, x, x)\n",
    "\n",
    "        x = self.norm1(x + attention)\n",
    "\n",
    "        # print(x.shape)\n",
    "\n",
    "\n",
    "        print(x.shape)\n",
    "\n",
    "        # x = x.reshape(batch_size, lookback_size, self.embedding_size)\n",
    "        # print(x.shape)\n",
    "\n",
    "\n",
    "        # we mix the signals together\n",
    "        x = self.flat2(x)\n",
    "        x = self.head(x)\n",
    "        x = self.unflat2(x)\n",
    "\n",
    "        # print(x.shape)\n",
    "\n",
    "        \n",
    "        # x = self.flat(x)\n",
    "        # x, _ = self.lstm(x)\n",
    "        # # print(x.shape)\n",
    "        # x = self.flat(x)\n",
    "        # # print(x.shape)\n",
    "        # x = self.linear(x)\n",
    "        # x = self.unflat(x)\n",
    "        # # print(x.shape)\n",
    "\n",
    "        # we stop for debugging\n",
    "        # print('stopping')\n",
    "        # raise Exception('stop')\n",
    "        return x\n",
    "\n",
    "\n",
    "model = AirModel(input_size=62, lookback_size=lookback)\n",
    "y_pred = model(X_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "The Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "batch_size = 16\n",
    "model = AirModel(input_size=62, lookback_size=lookback, device = device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=.01)\n",
    "\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "# we want to use the ranger optimizer\n",
    "# optimizer = optim.RAdam(model.parameters())\n",
    "loss_fn = nn.MSELoss()\n",
    "# we need to gradient clip because the LSTM is very sensitive to exploding gradients\n",
    "# https://stackoverflow.com/questions/54716377/pytorch-why-do-we-need-to-set-the-gradients-manually-to-zero-in-pytorch\n",
    "# https://stackoverflow.com/questions/55735643/how-to-use-clip-grad-norm-in-pytorch\n",
    "loader = data.DataLoader(data.TensorDataset(X_train, y_train), shuffle=True, batch_size=batch_size)\n",
    "\n",
    "avg_train_loss = []\n",
    "avg_test_loss = []\n",
    "\n",
    "n_epochs = 10\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in loader:\n",
    "        y_pred = model(X_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # Validation\n",
    "    # if epoch % 100 != 0:\n",
    "    #     continue\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_train_pred = model(X_train).cpu()\n",
    "        train_rmse = np.sqrt(loss_fn(y_train_pred, y_train.cpu()))\n",
    "        y_test_pred = model(X_test).cpu()\n",
    "        test_rmse = np.sqrt(loss_fn(y_test_pred, y_test.cpu()))\n",
    "    avg_train_loss.append(train_rmse)\n",
    "    avg_test_loss.append(test_rmse)\n",
    "    difference_train = avg_train_loss[-1] - avg_train_loss[-2] if len(avg_train_loss) > 1 else 0\n",
    "    difference_test = avg_test_loss[-1] - avg_test_loss[-2] if len(avg_test_loss) > 1 else 0\n",
    "    print(\"Epoch %d: train RMSE %.4f, test RMSE %.4f, diff train %.4f, diff test %.4f\" % (epoch, train_rmse, test_rmse, difference_train, difference_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train RMSE 217281724416000.0000, test RMSE 249198532362240.0000, diff train -1459617792.0000, diff test -1644167168.0000\n",
      "Epoch 1: train RMSE 217280214466560.0000, test RMSE 249196821086208.0000, diff train -1509949440.0000, diff test -1711276032.0000\n",
      "Epoch 2: train RMSE 217278654185472.0000, test RMSE 249195093032960.0000, diff train -1560281088.0000, diff test -1728053248.0000\n",
      "Epoch 3: train RMSE 217277077127168.0000, test RMSE 249193297870848.0000, diff train -1577058304.0000, diff test -1795162112.0000\n",
      "Epoch 4: train RMSE 217275432960000.0000, test RMSE 249191485931520.0000, diff train -1644167168.0000, diff test -1811939328.0000\n",
      "Epoch 5: train RMSE 217273755238400.0000, test RMSE 249189606883328.0000, diff train -1677721600.0000, diff test -1879048192.0000\n",
      "Epoch 6: train RMSE 217272060739584.0000, test RMSE 249187694280704.0000, diff train -1694498816.0000, diff test -1912602624.0000\n",
      "Epoch 7: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 8: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 9: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 10: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 11: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 12: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 13: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 14: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 15: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 16: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 17: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 18: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 19: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 20: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 21: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 22: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 23: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 24: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 25: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 26: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 27: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 28: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 29: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 30: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 31: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 32: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 33: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 34: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 35: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 36: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 37: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 38: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 39: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 40: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 41: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 42: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 43: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 44: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 45: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 46: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 47: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 48: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 49: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 50: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 51: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 52: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 53: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 54: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 55: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 56: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 57: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 58: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 59: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 60: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 61: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 62: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 63: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 64: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 65: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 66: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 67: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 68: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 69: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 70: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 71: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 72: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 73: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 74: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 75: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 76: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 77: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 78: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 79: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n",
      "Epoch 80: train RMSE nan, test RMSE nan, diff train nan, diff test nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[157], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[0;32m----> 7\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(y_pred, y_batch)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fast/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[141], line 38\u001b[0m, in \u001b[0;36mAirModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# print(x.shape)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# x = self.cont_embed(x)\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcont_embed(x) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embed(torch\u001b[38;5;241m.\u001b[39marange(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m---> 38\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# we mix the signals together\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflat(x)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fast/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fast/lib/python3.10/site-packages/torch/nn/modules/normalization.py:189\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 189\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlayer_norm(\n\u001b[1;32m    190\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnormalized_shape, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fast/lib/python3.10/site-packages/torch/nn/functional.py:2503\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2499\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight, bias):\n\u001b[1;32m   2500\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2501\u001b[0m         layer_norm, (\u001b[39minput\u001b[39m, weight, bias), \u001b[39minput\u001b[39m, normalized_shape, weight\u001b[39m=\u001b[39mweight, bias\u001b[39m=\u001b[39mbias, eps\u001b[39m=\u001b[39meps\n\u001b[1;32m   2502\u001b[0m     )\n\u001b[0;32m-> 2503\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mlayer_norm(\u001b[39minput\u001b[39;49m, normalized_shape, weight, bias, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# batch_size = 64\n",
    "# loader = data.DataLoader(data.TensorDataset(X_train, y_train), shuffle=True, batch_size=batch_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=.1)\n",
    "n_epochs = 50\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in loader:\n",
    "        y_pred = model(X_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # Validation\n",
    "    # if epoch % 100 != 0:\n",
    "    #     continue\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_train_pred = model(X_train).cpu()\n",
    "        train_rmse = np.sqrt(loss_fn(y_train_pred, y_train.cpu()))\n",
    "        y_test_pred = model(X_test).cpu()\n",
    "        test_rmse = np.sqrt(loss_fn(y_test_pred, y_test.cpu()))\n",
    "    avg_train_loss.append(train_rmse)\n",
    "    avg_test_loss.append(test_rmse)\n",
    "    difference_train = avg_train_loss[-1] - avg_train_loss[-2] if len(avg_train_loss) > 1 else 0\n",
    "    difference_test = avg_test_loss[-1] - avg_test_loss[-2] if len(avg_test_loss) > 1 else 0\n",
    "    print(\"Epoch %d: train RMSE %.4f, test RMSE %.4f, diff train %.4f, diff test %.4f\" % (epoch, train_rmse, test_rmse, difference_train, difference_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train RMSE nan, test RMSE nan, diff train 0.0000, diff test 0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[150], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\u001b[39;00m\n\u001b[1;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fast/lib/python3.10/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fast/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "batch_size = 32\n",
    "model = AirModel(input_size=62, lookback_size=lookback)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "# we want to use the ranger optimizer\n",
    "# optimizer = optim.RAdam(model.parameters())\n",
    "loss_fn = nn.MSELoss()\n",
    "# we need to gradient clip because the LSTM is very sensitive to exploding gradients\n",
    "# https://stackoverflow.com/questions/54716377/pytorch-why-do-we-need-to-set-the-gradients-manually-to-zero-in-pytorch\n",
    "# https://stackoverflow.com/questions/55735643/how-to-use-clip-grad-norm-in-pytorch\n",
    "loader = data.DataLoader(data.TensorDataset(X_train, y_train), shuffle=True, batch_size=batch_size)\n",
    "\n",
    "avg_train_loss = []\n",
    "avg_test_loss = []\n",
    "\n",
    "n_epochs = 20\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in loader:\n",
    "        y_pred = model(X_batch)\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.zero_grad()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "    # Validation\n",
    "    # if epoch % 100 != 0:\n",
    "    #     continue\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_train)\n",
    "        train_rmse = np.sqrt(loss_fn(y_pred, y_train))\n",
    "        y_pred = model(X_test)\n",
    "        test_rmse = np.sqrt(loss_fn(y_pred, y_test))\n",
    "    avg_train_loss.append(train_rmse)\n",
    "    avg_test_loss.append(test_rmse)\n",
    "    difference_train = avg_train_loss[-1] - avg_train_loss[-2] if len(avg_train_loss) > 1 else 0\n",
    "    difference_test = avg_test_loss[-1] - avg_test_loss[-2] if len(avg_test_loss) > 1 else 0\n",
    "    print(\"Epoch %d: train RMSE %.4f, test RMSE %.4f, diff train %.4f, diff test %.4f\" % (epoch, train_rmse, test_rmse, difference_train, difference_test))\n",
    "    #  train avg %.1f, test avg %.1f\" % (epoch, train_rmse, test_rmse, np.mean(avg_train_loss), np.mean(avg_test_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train RMSE 227676518350848.0000, test RMSE 229166117027840.0000\n",
      "Epoch 1: train RMSE 227676283469824.0000, test RMSE 229165915701248.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[103], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[0;32m---> 20\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(y_pred, y_batch)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fast/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[97], line 36\u001b[0m, in \u001b[0;36mAirModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# x = self.cont_embed(x) + self.pos_embed(torch.arange(x.shape[1]))\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# we mix the signals together\u001b[39;00m\n\u001b[1;32m     35\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflat(x)\n\u001b[0;32m---> 36\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munflat(x)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# print(x.shape)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# print('stopping')\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# raise Exception('stop')\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fast/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fast/lib/python3.10/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fast/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fast/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "model = AirModel(input_size=62, lookback_size=lookback)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "# we want to use the ranger optimizer\n",
    "# optimizer = optim.RAdam(model.parameters())\n",
    "loss_fn = nn.MSELoss()\n",
    "# we need to gradient clip because the LSTM is very sensitive to exploding gradients\n",
    "# https://stackoverflow.com/questions/54716377/pytorch-why-do-we-need-to-set-the-gradients-manually-to-zero-in-pytorch\n",
    "# https://stackoverflow.com/questions/55735643/how-to-use-clip-grad-norm-in-pytorch\n",
    "loader = data.DataLoader(data.TensorDataset(X_train, y_train), shuffle=True, batch_size=8)\n",
    "\n",
    "n_epochs = 2000\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in loader:\n",
    "        y_pred = model(X_batch)\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # Validation\n",
    "    # if epoch % 100 != 0:\n",
    "    #     continue\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_train)\n",
    "        train_rmse = np.sqrt(loss_fn(y_pred, y_train))\n",
    "        y_pred = model(X_test)\n",
    "        test_rmse = np.sqrt(loss_fn(y_pred, y_test))\n",
    "    print(\"Epoch %d: train RMSE %.4f, test RMSE %.4f\" % (epoch, train_rmse, test_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train RMSE 225961433890816.0000, test RMSE 227331327131648.0000\n",
      "Epoch 1: train RMSE 225961433890816.0000, test RMSE 227331327131648.0000\n",
      "Epoch 2: train RMSE 225961433890816.0000, test RMSE 227331310354432.0000\n",
      "Epoch 3: train RMSE 225961433890816.0000, test RMSE 227331310354432.0000\n",
      "Epoch 4: train RMSE 225961433890816.0000, test RMSE 227331310354432.0000\n",
      "Epoch 5: train RMSE 225961433890816.0000, test RMSE 227331310354432.0000\n",
      "Epoch 6: train RMSE 225924171694080.0000, test RMSE 227295407112192.0000\n",
      "Epoch 7: train RMSE 225924171694080.0000, test RMSE 227295407112192.0000\n",
      "Epoch 8: train RMSE 225924154916864.0000, test RMSE 227295407112192.0000\n",
      "Epoch 9: train RMSE 225924154916864.0000, test RMSE 227295407112192.0000\n",
      "Epoch 10: train RMSE 225924154916864.0000, test RMSE 227295407112192.0000\n",
      "Epoch 11: train RMSE 225924154916864.0000, test RMSE 227295407112192.0000\n",
      "Epoch 12: train RMSE 225924154916864.0000, test RMSE 227295407112192.0000\n",
      "Epoch 13: train RMSE 225924154916864.0000, test RMSE 227295390334976.0000\n",
      "Epoch 14: train RMSE 225924154916864.0000, test RMSE 227295390334976.0000\n",
      "Epoch 15: train RMSE 225924154916864.0000, test RMSE 227295390334976.0000\n",
      "Epoch 16: train RMSE 225924154916864.0000, test RMSE 227295390334976.0000\n",
      "Epoch 17: train RMSE 225924205248512.0000, test RMSE 227295423889408.0000\n",
      "Epoch 18: train RMSE 225924205248512.0000, test RMSE 227295423889408.0000\n",
      "Epoch 19: train RMSE 225924205248512.0000, test RMSE 227295423889408.0000\n",
      "Epoch 20: train RMSE 225924205248512.0000, test RMSE 227295423889408.0000\n",
      "Epoch 21: train RMSE 225924205248512.0000, test RMSE 227295423889408.0000\n",
      "Epoch 22: train RMSE 225924205248512.0000, test RMSE 227295423889408.0000\n",
      "Epoch 23: train RMSE 225924188471296.0000, test RMSE 227295423889408.0000\n",
      "Epoch 24: train RMSE 225924188471296.0000, test RMSE 227295423889408.0000\n",
      "Epoch 25: train RMSE 225924188471296.0000, test RMSE 227295423889408.0000\n",
      "Epoch 26: train RMSE 225924188471296.0000, test RMSE 227295423889408.0000\n",
      "Epoch 27: train RMSE 225924188471296.0000, test RMSE 227295423889408.0000\n",
      "Epoch 28: train RMSE 225924171694080.0000, test RMSE 227295423889408.0000\n",
      "Epoch 29: train RMSE 225924171694080.0000, test RMSE 227295407112192.0000\n",
      "Epoch 30: train RMSE 225924171694080.0000, test RMSE 227295407112192.0000\n",
      "Epoch 31: train RMSE 225924171694080.0000, test RMSE 227295407112192.0000\n",
      "Epoch 32: train RMSE 225924154916864.0000, test RMSE 227295407112192.0000\n",
      "Epoch 33: train RMSE 225924154916864.0000, test RMSE 227295407112192.0000\n",
      "Epoch 34: train RMSE 225924154916864.0000, test RMSE 227295407112192.0000\n",
      "Epoch 35: train RMSE 225924154916864.0000, test RMSE 227295407112192.0000\n",
      "Epoch 36: train RMSE 225924154916864.0000, test RMSE 227295407112192.0000\n",
      "Epoch 37: train RMSE 225924154916864.0000, test RMSE 227295407112192.0000\n",
      "Epoch 38: train RMSE 225924154916864.0000, test RMSE 227295407112192.0000\n",
      "Epoch 39: train RMSE 225924154916864.0000, test RMSE 227295407112192.0000\n",
      "Epoch 40: train RMSE 225924154916864.0000, test RMSE 227295407112192.0000\n",
      "Epoch 41: train RMSE 225924154916864.0000, test RMSE 227295390334976.0000\n",
      "Epoch 42: train RMSE 225924154916864.0000, test RMSE 227295390334976.0000\n",
      "Epoch 43: train RMSE 225924154916864.0000, test RMSE 227295390334976.0000\n",
      "Epoch 44: train RMSE 225924154916864.0000, test RMSE 227295390334976.0000\n",
      "Epoch 45: train RMSE 225924138139648.0000, test RMSE 227295390334976.0000\n",
      "Epoch 46: train RMSE 225924138139648.0000, test RMSE 227295390334976.0000\n",
      "Epoch 47: train RMSE 225924138139648.0000, test RMSE 227295373557760.0000\n",
      "Epoch 48: train RMSE 225924138139648.0000, test RMSE 227295373557760.0000\n",
      "Epoch 49: train RMSE 225924138139648.0000, test RMSE 227295373557760.0000\n",
      "Epoch 50: train RMSE 225924138139648.0000, test RMSE 227295373557760.0000\n",
      "Epoch 51: train RMSE 225924121362432.0000, test RMSE 227295373557760.0000\n",
      "Epoch 52: train RMSE 225924121362432.0000, test RMSE 227295373557760.0000\n",
      "Epoch 53: train RMSE 225924121362432.0000, test RMSE 227295373557760.0000\n",
      "Epoch 54: train RMSE 225924121362432.0000, test RMSE 227295373557760.0000\n",
      "Epoch 55: train RMSE 225924121362432.0000, test RMSE 227295373557760.0000\n",
      "Epoch 56: train RMSE 225924121362432.0000, test RMSE 227295373557760.0000\n",
      "Epoch 57: train RMSE 225924121362432.0000, test RMSE 227295373557760.0000\n",
      "Epoch 58: train RMSE 225924104585216.0000, test RMSE 227295356780544.0000\n",
      "Epoch 59: train RMSE 225924104585216.0000, test RMSE 227295356780544.0000\n",
      "Epoch 60: train RMSE 225924104585216.0000, test RMSE 227295356780544.0000\n",
      "Epoch 61: train RMSE 225924104585216.0000, test RMSE 227295356780544.0000\n",
      "Epoch 62: train RMSE 225924104585216.0000, test RMSE 227295356780544.0000\n",
      "Epoch 63: train RMSE 225924104585216.0000, test RMSE 227295356780544.0000\n",
      "Epoch 64: train RMSE 225924104585216.0000, test RMSE 227295340003328.0000\n",
      "Epoch 65: train RMSE 225924104585216.0000, test RMSE 227295340003328.0000\n",
      "Epoch 66: train RMSE 225924104585216.0000, test RMSE 227295340003328.0000\n",
      "Epoch 67: train RMSE 225924104585216.0000, test RMSE 227295340003328.0000\n",
      "Epoch 68: train RMSE 225924104585216.0000, test RMSE 227295340003328.0000\n",
      "Epoch 69: train RMSE 225924104585216.0000, test RMSE 227295340003328.0000\n",
      "Epoch 70: train RMSE 225924104585216.0000, test RMSE 227295323226112.0000\n",
      "Epoch 71: train RMSE 225924104585216.0000, test RMSE 227295323226112.0000\n",
      "Epoch 72: train RMSE 225924104585216.0000, test RMSE 227295323226112.0000\n",
      "Epoch 73: train RMSE 225924104585216.0000, test RMSE 227295323226112.0000\n",
      "Epoch 74: train RMSE 225924087808000.0000, test RMSE 227295323226112.0000\n",
      "Epoch 75: train RMSE 225924071030784.0000, test RMSE 227295323226112.0000\n",
      "Epoch 76: train RMSE 225924071030784.0000, test RMSE 227295323226112.0000\n",
      "Epoch 77: train RMSE 225924071030784.0000, test RMSE 227295323226112.0000\n",
      "Epoch 78: train RMSE 225924071030784.0000, test RMSE 227295323226112.0000\n",
      "Epoch 79: train RMSE 225924071030784.0000, test RMSE 227295323226112.0000\n",
      "Epoch 80: train RMSE 225924071030784.0000, test RMSE 227295323226112.0000\n",
      "Epoch 81: train RMSE 225924071030784.0000, test RMSE 227295323226112.0000\n",
      "Epoch 82: train RMSE 225924071030784.0000, test RMSE 227295306448896.0000\n",
      "Epoch 83: train RMSE 225924071030784.0000, test RMSE 227295306448896.0000\n",
      "Epoch 84: train RMSE 225924071030784.0000, test RMSE 227295306448896.0000\n",
      "Epoch 85: train RMSE 225924071030784.0000, test RMSE 227295306448896.0000\n",
      "Epoch 86: train RMSE 225924071030784.0000, test RMSE 227295306448896.0000\n",
      "Epoch 87: train RMSE 225924071030784.0000, test RMSE 227295306448896.0000\n",
      "Epoch 88: train RMSE 225924071030784.0000, test RMSE 227295306448896.0000\n",
      "Epoch 89: train RMSE 225924071030784.0000, test RMSE 227295289671680.0000\n",
      "Epoch 90: train RMSE 225924054253568.0000, test RMSE 227295289671680.0000\n",
      "Epoch 91: train RMSE 225924054253568.0000, test RMSE 227295289671680.0000\n",
      "Epoch 92: train RMSE 225924054253568.0000, test RMSE 227295289671680.0000\n",
      "Epoch 93: train RMSE 225924054253568.0000, test RMSE 227295289671680.0000\n",
      "Epoch 94: train RMSE 225924054253568.0000, test RMSE 227295289671680.0000\n",
      "Epoch 95: train RMSE 225924054253568.0000, test RMSE 227295289671680.0000\n",
      "Epoch 96: train RMSE 225924054253568.0000, test RMSE 227295289671680.0000\n",
      "Epoch 97: train RMSE 225924054253568.0000, test RMSE 227295289671680.0000\n",
      "Epoch 98: train RMSE 225924037476352.0000, test RMSE 227295289671680.0000\n",
      "Epoch 99: train RMSE 225924037476352.0000, test RMSE 227295289671680.0000\n",
      "Epoch 100: train RMSE 225924037476352.0000, test RMSE 227295272894464.0000\n",
      "Epoch 101: train RMSE 225924037476352.0000, test RMSE 227295272894464.0000\n",
      "Epoch 102: train RMSE 225924037476352.0000, test RMSE 227295272894464.0000\n",
      "Epoch 103: train RMSE 225924037476352.0000, test RMSE 227295272894464.0000\n",
      "Epoch 104: train RMSE 225924037476352.0000, test RMSE 227295272894464.0000\n",
      "Epoch 105: train RMSE 225924037476352.0000, test RMSE 227295272894464.0000\n",
      "Epoch 106: train RMSE 225924020699136.0000, test RMSE 227295272894464.0000\n",
      "Epoch 107: train RMSE 225924020699136.0000, test RMSE 227295272894464.0000\n",
      "Epoch 108: train RMSE 225924020699136.0000, test RMSE 227295272894464.0000\n",
      "Epoch 109: train RMSE 225924020699136.0000, test RMSE 227295272894464.0000\n",
      "Epoch 110: train RMSE 225924020699136.0000, test RMSE 227295272894464.0000\n",
      "Epoch 111: train RMSE 225924020699136.0000, test RMSE 227295272894464.0000\n",
      "Epoch 112: train RMSE 225924020699136.0000, test RMSE 227295272894464.0000\n",
      "Epoch 113: train RMSE 225924020699136.0000, test RMSE 227295272894464.0000\n",
      "Epoch 114: train RMSE 225924003921920.0000, test RMSE 227295272894464.0000\n",
      "Epoch 115: train RMSE 225924003921920.0000, test RMSE 227295256117248.0000\n",
      "Epoch 116: train RMSE 225924003921920.0000, test RMSE 227295256117248.0000\n",
      "Epoch 117: train RMSE 225924003921920.0000, test RMSE 227295256117248.0000\n",
      "Epoch 118: train RMSE 225924003921920.0000, test RMSE 227295256117248.0000\n",
      "Epoch 119: train RMSE 225924003921920.0000, test RMSE 227295256117248.0000\n",
      "Epoch 120: train RMSE 225924003921920.0000, test RMSE 227295256117248.0000\n",
      "Epoch 121: train RMSE 225923987144704.0000, test RMSE 227295256117248.0000\n",
      "Epoch 122: train RMSE 225923987144704.0000, test RMSE 227295256117248.0000\n",
      "Epoch 123: train RMSE 225923987144704.0000, test RMSE 227295239340032.0000\n",
      "Epoch 124: train RMSE 225923987144704.0000, test RMSE 227295239340032.0000\n",
      "Epoch 125: train RMSE 225923987144704.0000, test RMSE 227295222562816.0000\n",
      "Epoch 126: train RMSE 225923987144704.0000, test RMSE 227295222562816.0000\n",
      "Epoch 127: train RMSE 225923987144704.0000, test RMSE 227295222562816.0000\n",
      "Epoch 128: train RMSE 225923987144704.0000, test RMSE 227295222562816.0000\n",
      "Epoch 129: train RMSE 225923987144704.0000, test RMSE 227295222562816.0000\n",
      "Epoch 130: train RMSE 225923987144704.0000, test RMSE 227295222562816.0000\n",
      "Epoch 131: train RMSE 225923987144704.0000, test RMSE 227295222562816.0000\n",
      "Epoch 132: train RMSE 225923987144704.0000, test RMSE 227295205785600.0000\n",
      "Epoch 133: train RMSE 225923987144704.0000, test RMSE 227295205785600.0000\n",
      "Epoch 134: train RMSE 225923970367488.0000, test RMSE 227295205785600.0000\n",
      "Epoch 135: train RMSE 225923970367488.0000, test RMSE 227295205785600.0000\n",
      "Epoch 136: train RMSE 225923970367488.0000, test RMSE 227295205785600.0000\n",
      "Epoch 137: train RMSE 225923970367488.0000, test RMSE 227295205785600.0000\n",
      "Epoch 138: train RMSE 225923970367488.0000, test RMSE 227295205785600.0000\n",
      "Epoch 139: train RMSE 225923970367488.0000, test RMSE 227295205785600.0000\n",
      "Epoch 140: train RMSE 225923970367488.0000, test RMSE 227295205785600.0000\n",
      "Epoch 141: train RMSE 225923970367488.0000, test RMSE 227295205785600.0000\n",
      "Epoch 142: train RMSE 225923970367488.0000, test RMSE 227295205785600.0000\n",
      "Epoch 143: train RMSE 225923970367488.0000, test RMSE 227295205785600.0000\n",
      "Epoch 144: train RMSE 225923953590272.0000, test RMSE 227295205785600.0000\n",
      "Epoch 145: train RMSE 225923953590272.0000, test RMSE 227295205785600.0000\n",
      "Epoch 146: train RMSE 225923953590272.0000, test RMSE 227295189008384.0000\n",
      "Epoch 147: train RMSE 225923953590272.0000, test RMSE 227295189008384.0000\n",
      "Epoch 148: train RMSE 225923936813056.0000, test RMSE 227295189008384.0000\n",
      "Epoch 149: train RMSE 225923936813056.0000, test RMSE 227295189008384.0000\n",
      "Epoch 150: train RMSE 225923936813056.0000, test RMSE 227295189008384.0000\n",
      "Epoch 151: train RMSE 225923936813056.0000, test RMSE 227295189008384.0000\n",
      "Epoch 152: train RMSE 225923936813056.0000, test RMSE 227295189008384.0000\n",
      "Epoch 153: train RMSE 225923936813056.0000, test RMSE 227295189008384.0000\n",
      "Epoch 154: train RMSE 225923936813056.0000, test RMSE 227295189008384.0000\n",
      "Epoch 155: train RMSE 225923936813056.0000, test RMSE 227295189008384.0000\n",
      "Epoch 156: train RMSE 225923936813056.0000, test RMSE 227295189008384.0000\n",
      "Epoch 157: train RMSE 225923936813056.0000, test RMSE 227295189008384.0000\n",
      "Epoch 158: train RMSE 225923936813056.0000, test RMSE 227295189008384.0000\n",
      "Epoch 159: train RMSE 225923936813056.0000, test RMSE 227295189008384.0000\n",
      "Epoch 160: train RMSE 225923936813056.0000, test RMSE 227295155453952.0000\n",
      "Epoch 161: train RMSE 225923936813056.0000, test RMSE 227295155453952.0000\n",
      "Epoch 162: train RMSE 225923936813056.0000, test RMSE 227295155453952.0000\n",
      "Epoch 163: train RMSE 225923920035840.0000, test RMSE 227295155453952.0000\n",
      "Epoch 164: train RMSE 225923936813056.0000, test RMSE 227295155453952.0000\n",
      "Epoch 165: train RMSE 225923920035840.0000, test RMSE 227295155453952.0000\n",
      "Epoch 166: train RMSE 225923920035840.0000, test RMSE 227295155453952.0000\n",
      "Epoch 167: train RMSE 225923903258624.0000, test RMSE 227295155453952.0000\n",
      "Epoch 168: train RMSE 225923903258624.0000, test RMSE 227295155453952.0000\n",
      "Epoch 169: train RMSE 225923903258624.0000, test RMSE 227295155453952.0000\n",
      "Epoch 170: train RMSE 225923903258624.0000, test RMSE 227295155453952.0000\n",
      "Epoch 171: train RMSE 225923903258624.0000, test RMSE 227295155453952.0000\n",
      "Epoch 172: train RMSE 225923903258624.0000, test RMSE 227295155453952.0000\n",
      "Epoch 173: train RMSE 225923903258624.0000, test RMSE 227295138676736.0000\n",
      "Epoch 174: train RMSE 225923903258624.0000, test RMSE 227295138676736.0000\n",
      "Epoch 175: train RMSE 225923903258624.0000, test RMSE 227295138676736.0000\n",
      "Epoch 176: train RMSE 225923903258624.0000, test RMSE 227295138676736.0000\n",
      "Epoch 177: train RMSE 225923903258624.0000, test RMSE 227295138676736.0000\n",
      "Epoch 178: train RMSE 225923903258624.0000, test RMSE 227295138676736.0000\n",
      "Epoch 179: train RMSE 225923886481408.0000, test RMSE 227295138676736.0000\n",
      "Epoch 180: train RMSE 225923886481408.0000, test RMSE 227295138676736.0000\n",
      "Epoch 181: train RMSE 225923886481408.0000, test RMSE 227295138676736.0000\n",
      "Epoch 182: train RMSE 225923886481408.0000, test RMSE 227295138676736.0000\n",
      "Epoch 183: train RMSE 225923886481408.0000, test RMSE 227295138676736.0000\n",
      "Epoch 184: train RMSE 225923886481408.0000, test RMSE 227295138676736.0000\n",
      "Epoch 185: train RMSE 225923886481408.0000, test RMSE 227295138676736.0000\n",
      "Epoch 186: train RMSE 225923869704192.0000, test RMSE 227295138676736.0000\n",
      "Epoch 187: train RMSE 225923869704192.0000, test RMSE 227295138676736.0000\n",
      "Epoch 188: train RMSE 225923869704192.0000, test RMSE 227295138676736.0000\n",
      "Epoch 189: train RMSE 225923869704192.0000, test RMSE 227295121899520.0000\n",
      "Epoch 190: train RMSE 225923869704192.0000, test RMSE 227295121899520.0000\n",
      "Epoch 191: train RMSE 225923852926976.0000, test RMSE 227295121899520.0000\n",
      "Epoch 192: train RMSE 225923852926976.0000, test RMSE 227295105122304.0000\n",
      "Epoch 193: train RMSE 225923852926976.0000, test RMSE 227295105122304.0000\n",
      "Epoch 194: train RMSE 225923852926976.0000, test RMSE 227295105122304.0000\n",
      "Epoch 195: train RMSE 225923852926976.0000, test RMSE 227295105122304.0000\n",
      "Epoch 196: train RMSE 225923852926976.0000, test RMSE 227295105122304.0000\n",
      "Epoch 197: train RMSE 225923852926976.0000, test RMSE 227295088345088.0000\n",
      "Epoch 198: train RMSE 225923852926976.0000, test RMSE 227295088345088.0000\n",
      "Epoch 199: train RMSE 225923852926976.0000, test RMSE 227295088345088.0000\n",
      "Epoch 200: train RMSE 225923852926976.0000, test RMSE 227295088345088.0000\n",
      "Epoch 201: train RMSE 225923852926976.0000, test RMSE 227295088345088.0000\n",
      "Epoch 202: train RMSE 225923852926976.0000, test RMSE 227295088345088.0000\n",
      "Epoch 203: train RMSE 225923852926976.0000, test RMSE 227295088345088.0000\n",
      "Epoch 204: train RMSE 225923836149760.0000, test RMSE 227295088345088.0000\n",
      "Epoch 205: train RMSE 225923836149760.0000, test RMSE 227295088345088.0000\n",
      "Epoch 206: train RMSE 225923836149760.0000, test RMSE 227295071567872.0000\n",
      "Epoch 207: train RMSE 225923819372544.0000, test RMSE 227295071567872.0000\n",
      "Epoch 208: train RMSE 225923819372544.0000, test RMSE 227295071567872.0000\n",
      "Epoch 209: train RMSE 225923819372544.0000, test RMSE 227295071567872.0000\n",
      "Epoch 210: train RMSE 225923819372544.0000, test RMSE 227295071567872.0000\n",
      "Epoch 211: train RMSE 225923819372544.0000, test RMSE 227295071567872.0000\n",
      "Epoch 212: train RMSE 225923819372544.0000, test RMSE 227295071567872.0000\n",
      "Epoch 213: train RMSE 225923819372544.0000, test RMSE 227295071567872.0000\n",
      "Epoch 214: train RMSE 225923819372544.0000, test RMSE 227295071567872.0000\n",
      "Epoch 215: train RMSE 225923819372544.0000, test RMSE 227295071567872.0000\n",
      "Epoch 216: train RMSE 225923819372544.0000, test RMSE 227295071567872.0000\n",
      "Epoch 217: train RMSE 225923819372544.0000, test RMSE 227295071567872.0000\n",
      "Epoch 218: train RMSE 225923819372544.0000, test RMSE 227295054790656.0000\n",
      "Epoch 219: train RMSE 225923802595328.0000, test RMSE 227295054790656.0000\n",
      "Epoch 220: train RMSE 225923802595328.0000, test RMSE 227295054790656.0000\n",
      "Epoch 221: train RMSE 225923802595328.0000, test RMSE 227295054790656.0000\n",
      "Epoch 222: train RMSE 225923802595328.0000, test RMSE 227295054790656.0000\n",
      "Epoch 223: train RMSE 225923802595328.0000, test RMSE 227295054790656.0000\n",
      "Epoch 224: train RMSE 225923802595328.0000, test RMSE 227295054790656.0000\n",
      "Epoch 225: train RMSE 225923802595328.0000, test RMSE 227295054790656.0000\n",
      "Epoch 226: train RMSE 225923802595328.0000, test RMSE 227295054790656.0000\n",
      "Epoch 227: train RMSE 225923802595328.0000, test RMSE 227295054790656.0000\n",
      "Epoch 228: train RMSE 225923802595328.0000, test RMSE 227295054790656.0000\n",
      "Epoch 229: train RMSE 225923785818112.0000, test RMSE 227295054790656.0000\n",
      "Epoch 230: train RMSE 225923785818112.0000, test RMSE 227295038013440.0000\n",
      "Epoch 231: train RMSE 225923785818112.0000, test RMSE 227295021236224.0000\n",
      "Epoch 232: train RMSE 225923785818112.0000, test RMSE 227295021236224.0000\n",
      "Epoch 233: train RMSE 225923785818112.0000, test RMSE 227295021236224.0000\n",
      "Epoch 234: train RMSE 225923785818112.0000, test RMSE 227295021236224.0000\n",
      "Epoch 235: train RMSE 225923785818112.0000, test RMSE 227295021236224.0000\n",
      "Epoch 236: train RMSE 225923785818112.0000, test RMSE 227295021236224.0000\n",
      "Epoch 237: train RMSE 225923785818112.0000, test RMSE 227295021236224.0000\n",
      "Epoch 238: train RMSE 225923769040896.0000, test RMSE 227295021236224.0000\n",
      "Epoch 239: train RMSE 225923769040896.0000, test RMSE 227295021236224.0000\n",
      "Epoch 240: train RMSE 225923769040896.0000, test RMSE 227295021236224.0000\n",
      "Epoch 241: train RMSE 225923769040896.0000, test RMSE 227295021236224.0000\n",
      "Epoch 242: train RMSE 225923769040896.0000, test RMSE 227295021236224.0000\n",
      "Epoch 243: train RMSE 225923769040896.0000, test RMSE 227295021236224.0000\n",
      "Epoch 244: train RMSE 225923769040896.0000, test RMSE 227295021236224.0000\n",
      "Epoch 245: train RMSE 225923769040896.0000, test RMSE 227295021236224.0000\n",
      "Epoch 246: train RMSE 225923769040896.0000, test RMSE 227295004459008.0000\n",
      "Epoch 247: train RMSE 225923769040896.0000, test RMSE 227295004459008.0000\n",
      "Epoch 248: train RMSE 225923769040896.0000, test RMSE 227295004459008.0000\n",
      "Epoch 249: train RMSE 225923752263680.0000, test RMSE 227295004459008.0000\n",
      "Epoch 250: train RMSE 225923752263680.0000, test RMSE 227295004459008.0000\n",
      "Epoch 251: train RMSE 225923752263680.0000, test RMSE 227295004459008.0000\n",
      "Epoch 252: train RMSE 225923752263680.0000, test RMSE 227295004459008.0000\n",
      "Epoch 253: train RMSE 225923752263680.0000, test RMSE 227295004459008.0000\n",
      "Epoch 254: train RMSE 225923752263680.0000, test RMSE 227295004459008.0000\n",
      "Epoch 255: train RMSE 225923752263680.0000, test RMSE 227294987681792.0000\n",
      "Epoch 256: train RMSE 225923735486464.0000, test RMSE 227294987681792.0000\n",
      "Epoch 257: train RMSE 225923735486464.0000, test RMSE 227294987681792.0000\n",
      "Epoch 258: train RMSE 225923735486464.0000, test RMSE 227294987681792.0000\n",
      "Epoch 259: train RMSE 225923735486464.0000, test RMSE 227294987681792.0000\n",
      "Epoch 260: train RMSE 225923735486464.0000, test RMSE 227294987681792.0000\n",
      "Epoch 261: train RMSE 225923735486464.0000, test RMSE 227294987681792.0000\n",
      "Epoch 262: train RMSE 225923735486464.0000, test RMSE 227294987681792.0000\n",
      "Epoch 263: train RMSE 225923718709248.0000, test RMSE 227294987681792.0000\n",
      "Epoch 264: train RMSE 225923718709248.0000, test RMSE 227294987681792.0000\n",
      "Epoch 265: train RMSE 225923718709248.0000, test RMSE 227294970904576.0000\n",
      "Epoch 266: train RMSE 225923718709248.0000, test RMSE 227294970904576.0000\n",
      "Epoch 267: train RMSE 225923718709248.0000, test RMSE 227294970904576.0000\n",
      "Epoch 268: train RMSE 225923718709248.0000, test RMSE 227294970904576.0000\n",
      "Epoch 269: train RMSE 225923718709248.0000, test RMSE 227294970904576.0000\n",
      "Epoch 270: train RMSE 225923718709248.0000, test RMSE 227294954127360.0000\n",
      "Epoch 271: train RMSE 225923718709248.0000, test RMSE 227294954127360.0000\n",
      "Epoch 272: train RMSE 225923718709248.0000, test RMSE 227294954127360.0000\n",
      "Epoch 273: train RMSE 225923718709248.0000, test RMSE 227294954127360.0000\n",
      "Epoch 274: train RMSE 225923718709248.0000, test RMSE 227294954127360.0000\n",
      "Epoch 275: train RMSE 225923718709248.0000, test RMSE 227294954127360.0000\n",
      "Epoch 276: train RMSE 225923701932032.0000, test RMSE 227294954127360.0000\n",
      "Epoch 277: train RMSE 225923685154816.0000, test RMSE 227294954127360.0000\n",
      "Epoch 278: train RMSE 225923685154816.0000, test RMSE 227294937350144.0000\n",
      "Epoch 279: train RMSE 225923685154816.0000, test RMSE 227294937350144.0000\n",
      "Epoch 280: train RMSE 225923685154816.0000, test RMSE 227294937350144.0000\n",
      "Epoch 281: train RMSE 225923685154816.0000, test RMSE 227294937350144.0000\n",
      "Epoch 282: train RMSE 225923685154816.0000, test RMSE 227294937350144.0000\n",
      "Epoch 283: train RMSE 225923668377600.0000, test RMSE 227294937350144.0000\n",
      "Epoch 284: train RMSE 225923668377600.0000, test RMSE 227294937350144.0000\n",
      "Epoch 285: train RMSE 225923668377600.0000, test RMSE 227294937350144.0000\n",
      "Epoch 286: train RMSE 225923668377600.0000, test RMSE 227294937350144.0000\n",
      "Epoch 287: train RMSE 225923668377600.0000, test RMSE 227294937350144.0000\n",
      "Epoch 288: train RMSE 225923668377600.0000, test RMSE 227294937350144.0000\n",
      "Epoch 289: train RMSE 225923668377600.0000, test RMSE 227294920572928.0000\n",
      "Epoch 290: train RMSE 225923668377600.0000, test RMSE 227294920572928.0000\n",
      "Epoch 291: train RMSE 225923668377600.0000, test RMSE 227294920572928.0000\n",
      "Epoch 292: train RMSE 225923668377600.0000, test RMSE 227294920572928.0000\n",
      "Epoch 293: train RMSE 225923668377600.0000, test RMSE 227294920572928.0000\n",
      "Epoch 294: train RMSE 225923668377600.0000, test RMSE 227294920572928.0000\n",
      "Epoch 295: train RMSE 225923651600384.0000, test RMSE 227294920572928.0000\n",
      "Epoch 296: train RMSE 225923651600384.0000, test RMSE 227294920572928.0000\n",
      "Epoch 297: train RMSE 225923651600384.0000, test RMSE 227294920572928.0000\n",
      "Epoch 298: train RMSE 225923651600384.0000, test RMSE 227294920572928.0000\n",
      "Epoch 299: train RMSE 225923651600384.0000, test RMSE 227294920572928.0000\n",
      "Epoch 300: train RMSE 225923651600384.0000, test RMSE 227294920572928.0000\n",
      "Epoch 301: train RMSE 225923651600384.0000, test RMSE 227294920572928.0000\n",
      "Epoch 302: train RMSE 225923651600384.0000, test RMSE 227294920572928.0000\n",
      "Epoch 303: train RMSE 225923651600384.0000, test RMSE 227294920572928.0000\n",
      "Epoch 304: train RMSE 225923634823168.0000, test RMSE 227294920572928.0000\n",
      "Epoch 305: train RMSE 225923634823168.0000, test RMSE 227294887018496.0000\n",
      "Epoch 306: train RMSE 225923634823168.0000, test RMSE 227294887018496.0000\n",
      "Epoch 307: train RMSE 225923634823168.0000, test RMSE 227294887018496.0000\n",
      "Epoch 308: train RMSE 225923634823168.0000, test RMSE 227294887018496.0000\n",
      "Epoch 309: train RMSE 225923634823168.0000, test RMSE 227294887018496.0000\n",
      "Epoch 310: train RMSE 225923634823168.0000, test RMSE 227294887018496.0000\n",
      "Epoch 311: train RMSE 225923634823168.0000, test RMSE 227294887018496.0000\n",
      "Epoch 312: train RMSE 225923634823168.0000, test RMSE 227294887018496.0000\n",
      "Epoch 313: train RMSE 225923634823168.0000, test RMSE 227294887018496.0000\n",
      "Epoch 314: train RMSE 225923634823168.0000, test RMSE 227294887018496.0000\n",
      "Epoch 315: train RMSE 225923634823168.0000, test RMSE 227294887018496.0000\n",
      "Epoch 316: train RMSE 225923634823168.0000, test RMSE 227294887018496.0000\n",
      "Epoch 317: train RMSE 225923634823168.0000, test RMSE 227294870241280.0000\n",
      "Epoch 318: train RMSE 225923618045952.0000, test RMSE 227294870241280.0000\n",
      "Epoch 319: train RMSE 225923618045952.0000, test RMSE 227294870241280.0000\n",
      "Epoch 320: train RMSE 225923618045952.0000, test RMSE 227294870241280.0000\n",
      "Epoch 321: train RMSE 225923618045952.0000, test RMSE 227294870241280.0000\n",
      "Epoch 322: train RMSE 225923601268736.0000, test RMSE 227294870241280.0000\n",
      "Epoch 323: train RMSE 225923601268736.0000, test RMSE 227294870241280.0000\n",
      "Epoch 324: train RMSE 225923601268736.0000, test RMSE 227294853464064.0000\n",
      "Epoch 325: train RMSE 225923601268736.0000, test RMSE 227294853464064.0000\n",
      "Epoch 326: train RMSE 225923601268736.0000, test RMSE 227294836686848.0000\n",
      "Epoch 327: train RMSE 225923601268736.0000, test RMSE 227294836686848.0000\n",
      "Epoch 328: train RMSE 225923601268736.0000, test RMSE 227294836686848.0000\n",
      "Epoch 329: train RMSE 225923601268736.0000, test RMSE 227294836686848.0000\n",
      "Epoch 330: train RMSE 225923601268736.0000, test RMSE 227294836686848.0000\n",
      "Epoch 331: train RMSE 225923601268736.0000, test RMSE 227294836686848.0000\n",
      "Epoch 332: train RMSE 225923601268736.0000, test RMSE 227294836686848.0000\n",
      "Epoch 333: train RMSE 225923567714304.0000, test RMSE 227294836686848.0000\n",
      "Epoch 334: train RMSE 225923567714304.0000, test RMSE 227294836686848.0000\n",
      "Epoch 335: train RMSE 225923567714304.0000, test RMSE 227294836686848.0000\n",
      "Epoch 336: train RMSE 225923567714304.0000, test RMSE 227294836686848.0000\n",
      "Epoch 337: train RMSE 225923567714304.0000, test RMSE 227294836686848.0000\n",
      "Epoch 338: train RMSE 225923567714304.0000, test RMSE 227294836686848.0000\n",
      "Epoch 339: train RMSE 225923567714304.0000, test RMSE 227294836686848.0000\n",
      "Epoch 340: train RMSE 225923567714304.0000, test RMSE 227294836686848.0000\n",
      "Epoch 341: train RMSE 225923567714304.0000, test RMSE 227294836686848.0000\n",
      "Epoch 342: train RMSE 225923567714304.0000, test RMSE 227294819909632.0000\n",
      "Epoch 343: train RMSE 225923550937088.0000, test RMSE 227294819909632.0000\n",
      "Epoch 344: train RMSE 225923550937088.0000, test RMSE 227294819909632.0000\n",
      "Epoch 345: train RMSE 225923550937088.0000, test RMSE 227294803132416.0000\n",
      "Epoch 346: train RMSE 225923550937088.0000, test RMSE 227294803132416.0000\n",
      "Epoch 347: train RMSE 225923550937088.0000, test RMSE 227294803132416.0000\n",
      "Epoch 348: train RMSE 225923550937088.0000, test RMSE 227294803132416.0000\n",
      "Epoch 349: train RMSE 225923550937088.0000, test RMSE 227294803132416.0000\n",
      "Epoch 350: train RMSE 225923550937088.0000, test RMSE 227294803132416.0000\n",
      "Epoch 351: train RMSE 225923534159872.0000, test RMSE 227294803132416.0000\n",
      "Epoch 352: train RMSE 225923534159872.0000, test RMSE 227294803132416.0000\n",
      "Epoch 353: train RMSE 225923534159872.0000, test RMSE 227294803132416.0000\n",
      "Epoch 354: train RMSE 225923534159872.0000, test RMSE 227294803132416.0000\n",
      "Epoch 355: train RMSE 225923534159872.0000, test RMSE 227294803132416.0000\n",
      "Epoch 356: train RMSE 225923534159872.0000, test RMSE 227294803132416.0000\n",
      "Epoch 357: train RMSE 225923534159872.0000, test RMSE 227294786355200.0000\n",
      "Epoch 358: train RMSE 225923534159872.0000, test RMSE 227294786355200.0000\n",
      "Epoch 359: train RMSE 225923534159872.0000, test RMSE 227294786355200.0000\n",
      "Epoch 360: train RMSE 225923517382656.0000, test RMSE 227294786355200.0000\n",
      "Epoch 361: train RMSE 225923517382656.0000, test RMSE 227294786355200.0000\n",
      "Epoch 362: train RMSE 225923517382656.0000, test RMSE 227294786355200.0000\n",
      "Epoch 363: train RMSE 225923517382656.0000, test RMSE 227294786355200.0000\n",
      "Epoch 364: train RMSE 225923517382656.0000, test RMSE 227294786355200.0000\n",
      "Epoch 365: train RMSE 225923517382656.0000, test RMSE 227294769577984.0000\n",
      "Epoch 366: train RMSE 225923517382656.0000, test RMSE 227294769577984.0000\n",
      "Epoch 367: train RMSE 225923517382656.0000, test RMSE 227294769577984.0000\n",
      "Epoch 368: train RMSE 225923517382656.0000, test RMSE 227294769577984.0000\n",
      "Epoch 369: train RMSE 225923517382656.0000, test RMSE 227294769577984.0000\n",
      "Epoch 370: train RMSE 225923517382656.0000, test RMSE 227294769577984.0000\n",
      "Epoch 371: train RMSE 225923517382656.0000, test RMSE 227294769577984.0000\n",
      "Epoch 372: train RMSE 225923500605440.0000, test RMSE 227294769577984.0000\n",
      "Epoch 373: train RMSE 225923500605440.0000, test RMSE 227294769577984.0000\n",
      "Epoch 374: train RMSE 225923500605440.0000, test RMSE 227294769577984.0000\n",
      "Epoch 375: train RMSE 225923500605440.0000, test RMSE 227294769577984.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     21\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 22\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Validation\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# if epoch % 100 != 0:\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#     continue\u001b[39;00m\n\u001b[1;32m     26\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fast/lib/python3.10/site-packages/torch/optim/optimizer.py:111\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    109\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    110\u001b[0m     obj, \u001b[39m*\u001b[39m_ \u001b[39m=\u001b[39m args\n\u001b[0;32m--> 111\u001b[0m     profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39;49m\u001b[39mOptimizer.step#\u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m.step\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mformat(obj\u001b[39m.\u001b[39;49m\u001b[39m__class__\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m)\n\u001b[1;32m    112\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m    113\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "model = AirModel(input_size=62, lookback_size=lookback)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = nn.MSELoss()\n",
    "# we need to gradient clip because the LSTM is very sensitive to exploding gradients\n",
    "# https://stackoverflow.com/questions/54716377/pytorch-why-do-we-need-to-set-the-gradients-manually-to-zero-in-pytorch\n",
    "# https://stackoverflow.com/questions/55735643/how-to-use-clip-grad-norm-in-pytorch\n",
    "loader = data.DataLoader(data.TensorDataset(X_train, y_train), shuffle=True, batch_size=8)\n",
    "\n",
    "n_epochs = 2000\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in loader:\n",
    "        y_pred = model(X_batch)\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # Validation\n",
    "    # if epoch % 100 != 0:\n",
    "    #     continue\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_train)\n",
    "        train_rmse = np.sqrt(loss_fn(y_pred, y_train))\n",
    "        y_pred = model(X_test)\n",
    "        test_rmse = np.sqrt(loss_fn(y_pred, y_test))\n",
    "    print(\"Epoch %d: train RMSE %.4f, test RMSE %.4f\" % (epoch, train_rmse, test_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch 0: train RMSE 225961433890816.0000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we experiment with the ddos syn dataset. Unlike the darknet dataset, which was pre cleaned for a different experiment, the syn dataset has its timestamps, so it can be ordered by time. We will use this to our advantage to create a time series model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleting 282982 rows with Infinity in column Flow Bytes/s\n"
     ]
    }
   ],
   "source": [
    "ddos: pd.DataFrame = remove_infs_and_nans(dataset_3)\n",
    "ddos.columns = [column.strip() for column in ddos.columns]\n",
    "ddos = ddos.sort_values(by=['Timestamp'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "categorical_cols = [\n",
    "    'Protocol',\n",
    "    'Timestamp',\n",
    "    'Inbound',\n",
    "]\n",
    "\n",
    "unused_cols = [\n",
    "    'Unnamed: 0',\n",
    "    'Flow ID',\n",
    "    'Source IP',\n",
    "    'Destination IP',\n",
    "    'SimillarHTTP',\n",
    "    'Label'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Flow ID</th>\n",
       "      <th>Source IP</th>\n",
       "      <th>Source Port</th>\n",
       "      <th>Destination IP</th>\n",
       "      <th>Destination Port</th>\n",
       "      <th>Protocol</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Flow Duration</th>\n",
       "      <th>Total Fwd Packets</th>\n",
       "      <th>...</th>\n",
       "      <th>Active Std</th>\n",
       "      <th>Active Max</th>\n",
       "      <th>Active Min</th>\n",
       "      <th>Idle Mean</th>\n",
       "      <th>Idle Std</th>\n",
       "      <th>Idle Max</th>\n",
       "      <th>Idle Min</th>\n",
       "      <th>SimillarHTTP</th>\n",
       "      <th>Inbound</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>445444</td>\n",
       "      <td>172.16.0.5-192.168.50.4-9429-9429-6</td>\n",
       "      <td>172.16.0.5</td>\n",
       "      <td>9429</td>\n",
       "      <td>192.168.50.4</td>\n",
       "      <td>9429</td>\n",
       "      <td>6</td>\n",
       "      <td>2018-11-03 11:36:28.607338</td>\n",
       "      <td>36063894</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>29.444864</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.202128e+07</td>\n",
       "      <td>6.253623e+06</td>\n",
       "      <td>18628035.0</td>\n",
       "      <td>6193840.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Syn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>113842</td>\n",
       "      <td>172.16.0.5-192.168.50.4-60224-60224-6</td>\n",
       "      <td>172.16.0.5</td>\n",
       "      <td>60224</td>\n",
       "      <td>192.168.50.4</td>\n",
       "      <td>60224</td>\n",
       "      <td>6</td>\n",
       "      <td>2018-11-03 11:36:28.607339</td>\n",
       "      <td>44851366</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.066268e+07</td>\n",
       "      <td>1.169783e+07</td>\n",
       "      <td>28934293.0</td>\n",
       "      <td>12391060.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Syn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>176377</td>\n",
       "      <td>172.16.0.5-192.168.50.4-33827-11746-6</td>\n",
       "      <td>192.168.50.4</td>\n",
       "      <td>11746</td>\n",
       "      <td>172.16.0.5</td>\n",
       "      <td>33827</td>\n",
       "      <td>6</td>\n",
       "      <td>2018-11-03 11:36:28.607388</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Syn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>85100</td>\n",
       "      <td>172.16.0.5-192.168.50.4-5311-5311-6</td>\n",
       "      <td>172.16.0.5</td>\n",
       "      <td>5311</td>\n",
       "      <td>192.168.50.4</td>\n",
       "      <td>5311</td>\n",
       "      <td>6</td>\n",
       "      <td>2018-11-03 11:36:28.607442</td>\n",
       "      <td>35731470</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>33.234019</td>\n",
       "      <td>48.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.191047e+07</td>\n",
       "      <td>1.849493e+06</td>\n",
       "      <td>13693985.0</td>\n",
       "      <td>10001398.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Syn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>54739</td>\n",
       "      <td>172.16.0.5-192.168.50.4-33829-32787-6</td>\n",
       "      <td>172.16.0.5</td>\n",
       "      <td>33829</td>\n",
       "      <td>192.168.50.4</td>\n",
       "      <td>32787</td>\n",
       "      <td>6</td>\n",
       "      <td>2018-11-03 11:36:28.607492</td>\n",
       "      <td>101</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Syn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  88 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                Flow ID     Source IP  \\\n",
       "0      445444    172.16.0.5-192.168.50.4-9429-9429-6    172.16.0.5   \n",
       "1      113842  172.16.0.5-192.168.50.4-60224-60224-6    172.16.0.5   \n",
       "2      176377  172.16.0.5-192.168.50.4-33827-11746-6  192.168.50.4   \n",
       "4       85100    172.16.0.5-192.168.50.4-5311-5311-6    172.16.0.5   \n",
       "5       54739  172.16.0.5-192.168.50.4-33829-32787-6    172.16.0.5   \n",
       "\n",
       "   Source Port Destination IP  Destination Port  Protocol  \\\n",
       "0         9429   192.168.50.4              9429         6   \n",
       "1        60224   192.168.50.4             60224         6   \n",
       "2        11746     172.16.0.5             33827         6   \n",
       "4         5311   192.168.50.4              5311         6   \n",
       "5        33829   192.168.50.4             32787         6   \n",
       "\n",
       "                    Timestamp  Flow Duration  Total Fwd Packets  ...  \\\n",
       "0  2018-11-03 11:36:28.607338       36063894                  7  ...   \n",
       "1  2018-11-03 11:36:28.607339       44851366                  8  ...   \n",
       "2  2018-11-03 11:36:28.607388              1                  2  ...   \n",
       "4  2018-11-03 11:36:28.607442       35731470                  8  ...   \n",
       "5  2018-11-03 11:36:28.607492            101                  2  ...   \n",
       "\n",
       "   Active Std  Active Max  Active Min     Idle Mean      Idle Std    Idle Max  \\\n",
       "0   29.444864        52.0         1.0  1.202128e+07  6.253623e+06  18628035.0   \n",
       "1    0.000000         1.0         1.0  2.066268e+07  1.169783e+07  28934293.0   \n",
       "2    0.000000         0.0         0.0  0.000000e+00  0.000000e+00         0.0   \n",
       "4   33.234019        48.0         1.0  1.191047e+07  1.849493e+06  13693985.0   \n",
       "5    0.000000         0.0         0.0  0.000000e+00  0.000000e+00         0.0   \n",
       "\n",
       "     Idle Min  SimillarHTTP  Inbound  Label  \n",
       "0   6193840.0             0        1    Syn  \n",
       "1  12391060.0             0        1    Syn  \n",
       "2         0.0             0        0    Syn  \n",
       "4  10001398.0             0        1    Syn  \n",
       "5         0.0             0        1    Syn  \n",
       "\n",
       "[5 rows x 88 columns]"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Flow ID</th>\n",
       "      <th>Source IP</th>\n",
       "      <th>Source Port</th>\n",
       "      <th>Destination IP</th>\n",
       "      <th>Destination Port</th>\n",
       "      <th>Protocol</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Flow Duration</th>\n",
       "      <th>Total Fwd Packets</th>\n",
       "      <th>...</th>\n",
       "      <th>Active Std</th>\n",
       "      <th>Active Max</th>\n",
       "      <th>Active Min</th>\n",
       "      <th>Idle Mean</th>\n",
       "      <th>Idle Std</th>\n",
       "      <th>Idle Max</th>\n",
       "      <th>Idle Min</th>\n",
       "      <th>SimillarHTTP</th>\n",
       "      <th>Inbound</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>512583</th>\n",
       "      <td>333414</td>\n",
       "      <td>172.16.0.5-192.168.50.4-38138-56032-6</td>\n",
       "      <td>172.16.0.5</td>\n",
       "      <td>38138</td>\n",
       "      <td>192.168.50.4</td>\n",
       "      <td>56032</td>\n",
       "      <td>6</td>\n",
       "      <td>2018-11-03 11:30:25.830707</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Syn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512584</th>\n",
       "      <td>623775</td>\n",
       "      <td>172.16.0.5-192.168.50.4-38139-60562-6</td>\n",
       "      <td>172.16.0.5</td>\n",
       "      <td>38139</td>\n",
       "      <td>192.168.50.4</td>\n",
       "      <td>60562</td>\n",
       "      <td>6</td>\n",
       "      <td>2018-11-03 11:30:25.830709</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Syn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512585</th>\n",
       "      <td>625051</td>\n",
       "      <td>172.16.0.5-192.168.50.4-9122-9122-6</td>\n",
       "      <td>172.16.0.5</td>\n",
       "      <td>9122</td>\n",
       "      <td>192.168.50.4</td>\n",
       "      <td>9122</td>\n",
       "      <td>6</td>\n",
       "      <td>2018-11-03 11:30:25.830758</td>\n",
       "      <td>58882882</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>2.771281e+01</td>\n",
       "      <td>49.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18966142.0</td>\n",
       "      <td>6.576175e+06</td>\n",
       "      <td>24994571.0</td>\n",
       "      <td>11953226.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Syn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512586</th>\n",
       "      <td>506798</td>\n",
       "      <td>172.16.0.5-192.168.50.4-38140-17068-6</td>\n",
       "      <td>172.16.0.5</td>\n",
       "      <td>38140</td>\n",
       "      <td>192.168.50.4</td>\n",
       "      <td>17068</td>\n",
       "      <td>6</td>\n",
       "      <td>2018-11-03 11:30:25.830835</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Syn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512587</th>\n",
       "      <td>145397</td>\n",
       "      <td>172.16.0.5-192.168.50.4-19328-19328-6</td>\n",
       "      <td>172.16.0.5</td>\n",
       "      <td>19328</td>\n",
       "      <td>192.168.50.4</td>\n",
       "      <td>19328</td>\n",
       "      <td>6</td>\n",
       "      <td>2018-11-03 11:30:25.830837</td>\n",
       "      <td>64253671</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>2.226017e+06</td>\n",
       "      <td>4977574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9879316.5</td>\n",
       "      <td>3.544908e+06</td>\n",
       "      <td>14108721.0</td>\n",
       "      <td>6034220.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Syn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  88 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                                Flow ID   Source IP  \\\n",
       "512583      333414  172.16.0.5-192.168.50.4-38138-56032-6  172.16.0.5   \n",
       "512584      623775  172.16.0.5-192.168.50.4-38139-60562-6  172.16.0.5   \n",
       "512585      625051    172.16.0.5-192.168.50.4-9122-9122-6  172.16.0.5   \n",
       "512586      506798  172.16.0.5-192.168.50.4-38140-17068-6  172.16.0.5   \n",
       "512587      145397  172.16.0.5-192.168.50.4-19328-19328-6  172.16.0.5   \n",
       "\n",
       "        Source Port Destination IP  Destination Port  Protocol  \\\n",
       "512583        38138   192.168.50.4             56032         6   \n",
       "512584        38139   192.168.50.4             60562         6   \n",
       "512585         9122   192.168.50.4              9122         6   \n",
       "512586        38140   192.168.50.4             17068         6   \n",
       "512587        19328   192.168.50.4             19328         6   \n",
       "\n",
       "                         Timestamp  Flow Duration  Total Fwd Packets  ...  \\\n",
       "512583  2018-11-03 11:30:25.830707              1                  2  ...   \n",
       "512584  2018-11-03 11:30:25.830709             51                  2  ...   \n",
       "512585  2018-11-03 11:30:25.830758       58882882                 10  ...   \n",
       "512586  2018-11-03 11:30:25.830835              1                  2  ...   \n",
       "512587  2018-11-03 11:30:25.830837       64253671                 16  ...   \n",
       "\n",
       "          Active Std  Active Max  Active Min   Idle Mean      Idle Std  \\\n",
       "512583  0.000000e+00         0.0         0.0         0.0  0.000000e+00   \n",
       "512584  0.000000e+00         0.0         0.0         0.0  0.000000e+00   \n",
       "512585  2.771281e+01        49.0         1.0  18966142.0  6.576175e+06   \n",
       "512586  0.000000e+00         0.0         0.0         0.0  0.000000e+00   \n",
       "512587  2.226017e+06   4977574.0         1.0   9879316.5  3.544908e+06   \n",
       "\n",
       "          Idle Max    Idle Min  SimillarHTTP  Inbound  Label  \n",
       "512583         0.0         0.0             0        1    Syn  \n",
       "512584         0.0         0.0             0        1    Syn  \n",
       "512585  24994571.0  11953226.0             0        1    Syn  \n",
       "512586         0.0         0.0             0        1    Syn  \n",
       "512587  14108721.0   6034220.0             0        1    Syn  \n",
       "\n",
       "[5 rows x 88 columns]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddos = ddos.sort_values(by=['Timestamp'])\n",
    "ddos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Flow ID</th>\n",
       "      <th>Source IP</th>\n",
       "      <th>Source Port</th>\n",
       "      <th>Destination IP</th>\n",
       "      <th>Destination Port</th>\n",
       "      <th>Protocol</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Flow Duration</th>\n",
       "      <th>Total Fwd Packets</th>\n",
       "      <th>...</th>\n",
       "      <th>Active Std</th>\n",
       "      <th>Active Max</th>\n",
       "      <th>Active Min</th>\n",
       "      <th>Idle Mean</th>\n",
       "      <th>Idle Std</th>\n",
       "      <th>Idle Max</th>\n",
       "      <th>Idle Min</th>\n",
       "      <th>SimillarHTTP</th>\n",
       "      <th>Inbound</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>513853</th>\n",
       "      <td>448231</td>\n",
       "      <td>192.168.50.9-13.107.21.200-41640-443-6</td>\n",
       "      <td>13.107.21.200</td>\n",
       "      <td>443</td>\n",
       "      <td>192.168.50.9</td>\n",
       "      <td>41640</td>\n",
       "      <td>6</td>\n",
       "      <td>2018-11-03 11:30:25.923177</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520410</th>\n",
       "      <td>18711</td>\n",
       "      <td>8.0.6.4-8.6.0.1-0-0-0</td>\n",
       "      <td>8.6.0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0.6.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-11-03 11:30:26.404416</td>\n",
       "      <td>70496500</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.524825e+07</td>\n",
       "      <td>6.717942e+06</td>\n",
       "      <td>39998551.0</td>\n",
       "      <td>30497946.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530565</th>\n",
       "      <td>247443</td>\n",
       "      <td>192.168.10.50-192.168.50.9-22-49634-6</td>\n",
       "      <td>192.168.50.9</td>\n",
       "      <td>49634</td>\n",
       "      <td>192.168.10.50</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>2018-11-03 11:30:27.152582</td>\n",
       "      <td>63778896</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>4.158539e+06</td>\n",
       "      <td>7202825.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.885867e+07</td>\n",
       "      <td>1.226223e+07</td>\n",
       "      <td>32256047.0</td>\n",
       "      <td>8191976.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572658</th>\n",
       "      <td>361659</td>\n",
       "      <td>192.168.10.50-192.168.50.9-21-40416-6</td>\n",
       "      <td>192.168.50.9</td>\n",
       "      <td>40416</td>\n",
       "      <td>192.168.10.50</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>2018-11-03 11:30:30.259396</td>\n",
       "      <td>48384063</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.419203e+07</td>\n",
       "      <td>1.140422e+07</td>\n",
       "      <td>32256035.0</td>\n",
       "      <td>16128026.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576204</th>\n",
       "      <td>647026</td>\n",
       "      <td>192.168.50.253-224.0.0.5-0-0-0</td>\n",
       "      <td>192.168.50.253</td>\n",
       "      <td>0</td>\n",
       "      <td>224.0.0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-11-03 11:30:30.523542</td>\n",
       "      <td>65869716</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>4.879500e-01</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.409955e+06</td>\n",
       "      <td>3.903690e+05</td>\n",
       "      <td>9942643.0</td>\n",
       "      <td>9010337.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512573</th>\n",
       "      <td>7009</td>\n",
       "      <td>192.168.50.254-224.0.0.5-0-0-0</td>\n",
       "      <td>192.168.50.254</td>\n",
       "      <td>0</td>\n",
       "      <td>224.0.0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-11-03 17:36:07.740138</td>\n",
       "      <td>46841492</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>5.477226e-01</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.368293e+06</td>\n",
       "      <td>3.411952e+05</td>\n",
       "      <td>9942912.0</td>\n",
       "      <td>9069130.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512575</th>\n",
       "      <td>10248</td>\n",
       "      <td>192.168.50.6-65.55.252.93-58748-443-6</td>\n",
       "      <td>192.168.50.6</td>\n",
       "      <td>58748</td>\n",
       "      <td>65.55.252.93</td>\n",
       "      <td>443</td>\n",
       "      <td>6</td>\n",
       "      <td>2018-11-03 17:36:15.527613</td>\n",
       "      <td>9009178</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3008588.0</td>\n",
       "      <td>3008588.0</td>\n",
       "      <td>6.000587e+06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6000587.0</td>\n",
       "      <td>6000587.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512576</th>\n",
       "      <td>8404</td>\n",
       "      <td>192.168.50.253-224.0.0.5-0-0-0</td>\n",
       "      <td>192.168.50.253</td>\n",
       "      <td>0</td>\n",
       "      <td>224.0.0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-11-03 17:36:17.519669</td>\n",
       "      <td>38377507</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.594370e+06</td>\n",
       "      <td>3.083416e+05</td>\n",
       "      <td>9896301.0</td>\n",
       "      <td>9169376.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512580</th>\n",
       "      <td>13348</td>\n",
       "      <td>8.0.6.4-8.6.0.1-0-0-0</td>\n",
       "      <td>8.6.0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0.6.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-11-03 17:36:29.445702</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512581</th>\n",
       "      <td>12144</td>\n",
       "      <td>192.168.50.6-4.2.2.4-59165-53-17</td>\n",
       "      <td>192.168.50.6</td>\n",
       "      <td>59165</td>\n",
       "      <td>4.2.2.4</td>\n",
       "      <td>53</td>\n",
       "      <td>17</td>\n",
       "      <td>2018-11-03 17:36:33.932896</td>\n",
       "      <td>24646</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35387 rows  88 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                                 Flow ID       Source IP  \\\n",
       "513853      448231  192.168.50.9-13.107.21.200-41640-443-6   13.107.21.200   \n",
       "520410       18711                   8.0.6.4-8.6.0.1-0-0-0         8.6.0.1   \n",
       "530565      247443   192.168.10.50-192.168.50.9-22-49634-6    192.168.50.9   \n",
       "572658      361659   192.168.10.50-192.168.50.9-21-40416-6    192.168.50.9   \n",
       "576204      647026          192.168.50.253-224.0.0.5-0-0-0  192.168.50.253   \n",
       "...            ...                                     ...             ...   \n",
       "512573        7009          192.168.50.254-224.0.0.5-0-0-0  192.168.50.254   \n",
       "512575       10248   192.168.50.6-65.55.252.93-58748-443-6    192.168.50.6   \n",
       "512576        8404          192.168.50.253-224.0.0.5-0-0-0  192.168.50.253   \n",
       "512580       13348                   8.0.6.4-8.6.0.1-0-0-0         8.6.0.1   \n",
       "512581       12144        192.168.50.6-4.2.2.4-59165-53-17    192.168.50.6   \n",
       "\n",
       "        Source Port Destination IP  Destination Port  Protocol  \\\n",
       "513853          443   192.168.50.9             41640         6   \n",
       "520410            0        8.0.6.4                 0         0   \n",
       "530565        49634  192.168.10.50                22         6   \n",
       "572658        40416  192.168.10.50                21         6   \n",
       "576204            0      224.0.0.5                 0         0   \n",
       "...             ...            ...               ...       ...   \n",
       "512573            0      224.0.0.5                 0         0   \n",
       "512575        58748   65.55.252.93               443         6   \n",
       "512576            0      224.0.0.5                 0         0   \n",
       "512580            0        8.0.6.4                 0         0   \n",
       "512581        59165        4.2.2.4                53        17   \n",
       "\n",
       "                         Timestamp  Flow Duration  Total Fwd Packets  ...  \\\n",
       "513853  2018-11-03 11:30:25.923177              1                  2  ...   \n",
       "520410  2018-11-03 11:30:26.404416       70496500                  6  ...   \n",
       "530565  2018-11-03 11:30:27.152582       63778896                 14  ...   \n",
       "572658  2018-11-03 11:30:30.259396       48384063                  6  ...   \n",
       "576204  2018-11-03 11:30:30.523542       65869716                 32  ...   \n",
       "...                            ...            ...                ...  ...   \n",
       "512573  2018-11-03 17:36:07.740138       46841492                 18  ...   \n",
       "512575  2018-11-03 17:36:15.527613        9009178                  6  ...   \n",
       "512576  2018-11-03 17:36:17.519669       38377507                 20  ...   \n",
       "512580  2018-11-03 17:36:29.445702              2                  2  ...   \n",
       "512581  2018-11-03 17:36:33.932896          24646                  2  ...   \n",
       "\n",
       "          Active Std  Active Max  Active Min     Idle Mean      Idle Std  \\\n",
       "513853  0.000000e+00         0.0         0.0  0.000000e+00  0.000000e+00   \n",
       "520410  0.000000e+00         1.0         1.0  3.524825e+07  6.717942e+06   \n",
       "530565  4.158539e+06   7202825.0         1.0  1.885867e+07  1.226223e+07   \n",
       "572658  0.000000e+00         1.0         1.0  2.419203e+07  1.140422e+07   \n",
       "576204  4.879500e-01         4.0         3.0  9.409955e+06  3.903690e+05   \n",
       "...              ...         ...         ...           ...           ...   \n",
       "512573  5.477226e-01         5.0         4.0  9.368293e+06  3.411952e+05   \n",
       "512575  0.000000e+00   3008588.0   3008588.0  6.000587e+06  0.000000e+00   \n",
       "512576  5.000000e-01         6.0         5.0  9.594370e+06  3.083416e+05   \n",
       "512580  0.000000e+00         0.0         0.0  0.000000e+00  0.000000e+00   \n",
       "512581  0.000000e+00         0.0         0.0  0.000000e+00  0.000000e+00   \n",
       "\n",
       "          Idle Max    Idle Min  SimillarHTTP  Inbound   Label  \n",
       "513853         0.0         0.0             0        1  BENIGN  \n",
       "520410  39998551.0  30497946.0             0        0  BENIGN  \n",
       "530565  32256047.0   8191976.0             0        1  BENIGN  \n",
       "572658  32256035.0  16128026.0             0        1  BENIGN  \n",
       "576204   9942643.0   9010337.0             0        0  BENIGN  \n",
       "...            ...         ...           ...      ...     ...  \n",
       "512573   9942912.0   9069130.0             0        0  BENIGN  \n",
       "512575   6000587.0   6000587.0             0        0  BENIGN  \n",
       "512576   9896301.0   9169376.0             0        0  BENIGN  \n",
       "512580         0.0         0.0             0        0  BENIGN  \n",
       "512581         0.0         0.0             0        0  BENIGN  \n",
       "\n",
       "[35387 rows x 88 columns]"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddos[ddos[\"Label\"] != 'Syn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "benign = ddos[ddos[\"Label\"] == 'BENIGN']\n",
    "syn = ddos[ddos[\"Label\"] == 'Syn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65535"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_port = max(ddos['Destination Port'].max(), ddos['Source Port'].max())\n",
    "max_port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Flow ID</th>\n",
       "      <th>Source IP</th>\n",
       "      <th>Source Port</th>\n",
       "      <th>Destination IP</th>\n",
       "      <th>Destination Port</th>\n",
       "      <th>Protocol</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Flow Duration</th>\n",
       "      <th>Total Fwd Packets</th>\n",
       "      <th>...</th>\n",
       "      <th>Active Std</th>\n",
       "      <th>Active Max</th>\n",
       "      <th>Active Min</th>\n",
       "      <th>Idle Mean</th>\n",
       "      <th>Idle Std</th>\n",
       "      <th>Idle Max</th>\n",
       "      <th>Idle Min</th>\n",
       "      <th>SimillarHTTP</th>\n",
       "      <th>Inbound</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>512583</th>\n",
       "      <td>333414</td>\n",
       "      <td>172.16.0.5-192.168.50.4-38138-56032-6</td>\n",
       "      <td>172.16.0.5</td>\n",
       "      <td>38138</td>\n",
       "      <td>192.168.50.4</td>\n",
       "      <td>56032</td>\n",
       "      <td>6</td>\n",
       "      <td>2018-11-03 11:30:25.830707</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Syn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512584</th>\n",
       "      <td>623775</td>\n",
       "      <td>172.16.0.5-192.168.50.4-38139-60562-6</td>\n",
       "      <td>172.16.0.5</td>\n",
       "      <td>38139</td>\n",
       "      <td>192.168.50.4</td>\n",
       "      <td>60562</td>\n",
       "      <td>6</td>\n",
       "      <td>2018-11-03 11:30:25.830709</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Syn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512585</th>\n",
       "      <td>625051</td>\n",
       "      <td>172.16.0.5-192.168.50.4-9122-9122-6</td>\n",
       "      <td>172.16.0.5</td>\n",
       "      <td>9122</td>\n",
       "      <td>192.168.50.4</td>\n",
       "      <td>9122</td>\n",
       "      <td>6</td>\n",
       "      <td>2018-11-03 11:30:25.830758</td>\n",
       "      <td>58882882</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>2.771281e+01</td>\n",
       "      <td>49.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18966142.0</td>\n",
       "      <td>6.576175e+06</td>\n",
       "      <td>24994571.0</td>\n",
       "      <td>11953226.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Syn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512586</th>\n",
       "      <td>506798</td>\n",
       "      <td>172.16.0.5-192.168.50.4-38140-17068-6</td>\n",
       "      <td>172.16.0.5</td>\n",
       "      <td>38140</td>\n",
       "      <td>192.168.50.4</td>\n",
       "      <td>17068</td>\n",
       "      <td>6</td>\n",
       "      <td>2018-11-03 11:30:25.830835</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Syn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512587</th>\n",
       "      <td>145397</td>\n",
       "      <td>172.16.0.5-192.168.50.4-19328-19328-6</td>\n",
       "      <td>172.16.0.5</td>\n",
       "      <td>19328</td>\n",
       "      <td>192.168.50.4</td>\n",
       "      <td>19328</td>\n",
       "      <td>6</td>\n",
       "      <td>2018-11-03 11:30:25.830837</td>\n",
       "      <td>64253671</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>2.226017e+06</td>\n",
       "      <td>4977574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9879316.5</td>\n",
       "      <td>3.544908e+06</td>\n",
       "      <td>14108721.0</td>\n",
       "      <td>6034220.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Syn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  88 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                                Flow ID   Source IP  \\\n",
       "512583      333414  172.16.0.5-192.168.50.4-38138-56032-6  172.16.0.5   \n",
       "512584      623775  172.16.0.5-192.168.50.4-38139-60562-6  172.16.0.5   \n",
       "512585      625051    172.16.0.5-192.168.50.4-9122-9122-6  172.16.0.5   \n",
       "512586      506798  172.16.0.5-192.168.50.4-38140-17068-6  172.16.0.5   \n",
       "512587      145397  172.16.0.5-192.168.50.4-19328-19328-6  172.16.0.5   \n",
       "\n",
       "        Source Port Destination IP  Destination Port  Protocol  \\\n",
       "512583        38138   192.168.50.4             56032         6   \n",
       "512584        38139   192.168.50.4             60562         6   \n",
       "512585         9122   192.168.50.4              9122         6   \n",
       "512586        38140   192.168.50.4             17068         6   \n",
       "512587        19328   192.168.50.4             19328         6   \n",
       "\n",
       "                         Timestamp  Flow Duration  Total Fwd Packets  ...  \\\n",
       "512583  2018-11-03 11:30:25.830707              1                  2  ...   \n",
       "512584  2018-11-03 11:30:25.830709             51                  2  ...   \n",
       "512585  2018-11-03 11:30:25.830758       58882882                 10  ...   \n",
       "512586  2018-11-03 11:30:25.830835              1                  2  ...   \n",
       "512587  2018-11-03 11:30:25.830837       64253671                 16  ...   \n",
       "\n",
       "          Active Std  Active Max  Active Min   Idle Mean      Idle Std  \\\n",
       "512583  0.000000e+00         0.0         0.0         0.0  0.000000e+00   \n",
       "512584  0.000000e+00         0.0         0.0         0.0  0.000000e+00   \n",
       "512585  2.771281e+01        49.0         1.0  18966142.0  6.576175e+06   \n",
       "512586  0.000000e+00         0.0         0.0         0.0  0.000000e+00   \n",
       "512587  2.226017e+06   4977574.0         1.0   9879316.5  3.544908e+06   \n",
       "\n",
       "          Idle Max    Idle Min  SimillarHTTP  Inbound  Label  \n",
       "512583         0.0         0.0             0        1    Syn  \n",
       "512584         0.0         0.0             0        1    Syn  \n",
       "512585  24994571.0  11953226.0             0        1    Syn  \n",
       "512586         0.0         0.0             0        1    Syn  \n",
       "512587  14108721.0   6034220.0             0        1    Syn  \n",
       "\n",
       "[5 rows x 88 columns]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('fast_gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "deaf3a77c670ae4bcde0102b8754366ef6094e7b1a1d30917591bfafe6a4ef0f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
